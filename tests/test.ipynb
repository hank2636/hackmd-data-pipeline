{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60c23e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取環境變數\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "sys.path.append(str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import gzip\n",
    "import io\n",
    "import boto3\n",
    "import yaml\n",
    "from datetime import datetime, timezone\n",
    "from src.core.db import get_pg\n",
    "from src.core.pg_engine import PsqlEngine\n",
    "\n",
    "pg = get_pg()\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "logging.getLogger(\"boto3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"botocore\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"arxiv\").setLevel(logging.WARNING)\n",
    "\n",
    "def load_config(path=\"../config/config.yaml\"):\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "cfg = load_config()\n",
    "    \n",
    "MAX_RESULTS_GOAL = cfg[\"source_papers\"][\"max_results_goal\"]\n",
    "BATCH_SIZE = cfg[\"source_papers\"][\"batch_size\"]\n",
    "S3_BUCKET = cfg[\"aws\"][\"s3_bucket\"]\n",
    "MAX_ATTEMPTS = cfg[\"source_papers\"][\"s3_max_attempts\"]\n",
    "INITIAL_DELAY_SECONDS = cfg[\"source_papers\"][\"initial_delay_seconds\"]\n",
    "LOOKBACK_MONTHS = cfg[\"source_papers\"][\"lookback_months\"]\n",
    "\n",
    "client = arxiv.Client(\n",
    "    page_size=MAX_RESULTS_GOAL, \n",
    "    delay_seconds=3,\n",
    "    num_retries=3\n",
    ")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "\n",
    "def flatten_categories(cfg):\n",
    "    \"\"\"所有領域的 list\"\"\"\n",
    "    cats = []\n",
    "    for _, sub in cfg[\"categories\"].items():\n",
    "        cats.extend(sub)\n",
    "    return cats\n",
    "\n",
    "def select_next_categories(pending_categories, per_run):\n",
    "    \"\"\"\n",
    "    從待抓領域列表中選出本次要抓的 batch\n",
    "    pending_categories: list, 還沒完成的領域\n",
    "    per_run: int, 每次 Lambda 要抓多少個領域\n",
    "    return : batch_list\n",
    "    \"\"\"\n",
    "    if not pending_categories:\n",
    "        return []\n",
    "    return pending_categories[:per_run]\n",
    "\n",
    "def get_existing_categories():\n",
    "    \"\"\"讀取 DB 中已存在的所有領域\"\"\"\n",
    "    stmt = \"SELECT category_name, status FROM papers.category_progress\"\n",
    "    rows = pg.execute_query(stmt)\n",
    "    return {r[0]: r[1] for r in rows}\n",
    "\n",
    "def insert_new_categories(new_cats):\n",
    "    \"\"\"把 YAML 新增的領域寫進 DB\"\"\"\n",
    "    if not new_cats:\n",
    "        return\n",
    "    values = [(cat, '') for cat in new_cats]\n",
    "    pg.insert_mogrify(\"papers.category_progress\", values)\n",
    "\n",
    "def get_pending_categories():\n",
    "    \"\"\"取得還沒做完的領域\"\"\"\n",
    "    stmt = \"SELECT category_name FROM papers.category_progress WHERE status != 'Finished'\"\n",
    "    rows = pg.execute_query(stmt)\n",
    "    return [r[0] for r in rows]\n",
    "\n",
    "def mark_category_finished(category):\n",
    "    \"\"\"完成一個領域後更新狀態\"\"\"\n",
    "    stmt = \"\"\"\n",
    "        UPDATE papers.category_progress\n",
    "        SET status = 'Finished', updated_at = NOW()\n",
    "        WHERE category_name = %s\n",
    "    \"\"\"\n",
    "    pg.execute_cmd(stmt, (category,))\n",
    "  \n",
    "def insert_category_stats(category_stats):\n",
    "    \"\"\"\n",
    "    將 category_stats 批次寫入 papers.category_run_stats\n",
    "    使用 insert_mogrify\n",
    "    \"\"\"\n",
    "    if not category_stats:\n",
    "        return\n",
    "\n",
    "    utc_now = datetime.now(timezone.utc)\n",
    "    values = []\n",
    "    for cat, stats in category_stats.items():\n",
    "        values.append((\n",
    "            cat,\n",
    "            stats[\"time_sec\"],\n",
    "            stats[\"s3_count\"],\n",
    "            stats[\"pg_count\"],\n",
    "            utc_now\n",
    "        ))\n",
    "\n",
    "    pg.insert_mogrify(\"papers.category_run_stats\", values)\n",
    "    \n",
    "def load_existing_ids(months: int = LOOKBACK_MONTHS):\n",
    "    \"\"\"\n",
    "    只抓最近 N 個月內的 entry_id, 防 lambda 記憶體不足\n",
    "    預設 6 個月, 在 config.yaml 裡面有設\n",
    "    \"\"\"\n",
    "    stmt = f\"\"\"\n",
    "        SELECT entry_id\n",
    "        FROM papers.downloaded_papers\n",
    "        WHERE last_attempt >= NOW() - INTERVAL '{months} months'\n",
    "    \"\"\"\n",
    "    rows = pg.execute_query(stmt)\n",
    "    return set(r[0] for r in rows)\n",
    "\n",
    "\n",
    "def add_to_pg_batch(entry_id, category, status, etl_status, etl_batch_id=None, error_msg=\"\"):\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    pg_batch.append((entry_id, category, status, now_utc, error_msg, etl_status, etl_batch_id))\n",
    "\n",
    "\n",
    "def flush_pg_batch():\n",
    "    global pg_batch\n",
    "    if not pg_batch:\n",
    "        return\n",
    "    try:\n",
    "        pg.insert_mogrify(\"papers.downloaded_papers\", pg_batch)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to insert batch into Postgres: {e}\")\n",
    "    finally:\n",
    "        pg_batch = []\n",
    "        \n",
    "def add_raw_batches_to_pg(batch_id, category, s3_path, record_count, ):\n",
    "    stmt = \"\"\"\n",
    "        INSERT INTO etl.raw_batches (batch_id, category, s3_path, record_count)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "        ON CONFLICT (batch_id) DO NOTHING;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pg.execute_cmd(stmt, (batch_id, category, s3_path, record_count))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to insert into raw_batches: {e}\")\n",
    "\n",
    "def upload_batch_to_s3(s3_prefix, batch_data, batch_num, category):\n",
    "    if not batch_data:\n",
    "        return\n",
    "    jsonl_content = \"\\n\".join([json.dumps(paper, ensure_ascii=False) for paper in batch_data])\n",
    "    buffer = io.BytesIO()\n",
    "    with gzip.GzipFile(fileobj=buffer, mode='wb') as f:\n",
    "        f.write(jsonl_content.encode('utf-8'))\n",
    "    gzip_bytes = buffer.getvalue()\n",
    "    \n",
    "    utc_now = datetime.now(timezone.utc)\n",
    "    today_str = utc_now.strftime(\"%Y-%m-%d\")\n",
    "    utc_timestamp = int(utc_now.timestamp())\n",
    "    s3_key = f\"{s3_prefix}{today_str}/{category.replace('.','_')}_batch_{batch_num}_{utc_timestamp}.jsonl.gz\"\n",
    "    \n",
    "    last_exception = None\n",
    "    for attempt in range(MAX_ATTEMPTS):\n",
    "        try:\n",
    "            s3.put_object(\n",
    "                Bucket=S3_BUCKET,\n",
    "                Key=s3_key,\n",
    "                Body=gzip_bytes,\n",
    "                ContentType='application/json',\n",
    "                ContentEncoding='gzip'\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_exception = e\n",
    "            if attempt < MAX_ATTEMPTS - 1:\n",
    "                time.sleep(INITIAL_DELAY_SECONDS * (2 ** attempt))\n",
    "            else:\n",
    "                raise last_exception\n",
    "    return s3_key\n",
    "\n",
    "num_per_run = cfg[\"lambda\"][\"num_categories_per_run\"]\n",
    "all_categories = flatten_categories(cfg)  # YAML 裡所有領域\n",
    "existing_cats = get_existing_categories()\n",
    "\n",
    "new_cats = [cat for cat in all_categories if cat not in existing_cats]\n",
    "if new_cats:\n",
    "    insert_new_categories(new_cats)\n",
    "    print(f\"新增領域到 DB: {new_cats}\")\n",
    "\n",
    "\n",
    "pending_cats = get_pending_categories()\n",
    "if not pending_cats:\n",
    "    print(\"All categories are finished.\")\n",
    "    exit()\n",
    "\n",
    "category_list = select_next_categories(pending_cats, num_per_run)\n",
    "\n",
    "print(f\"這次執行的領域：{category_list}\")\n",
    "    \n",
    "existing_ids = load_existing_ids()\n",
    "category_stats = {}\n",
    "for category in category_list:\n",
    "    start_time = time.time()\n",
    "    s3_count = 0\n",
    "    pg_count = 0\n",
    "    try:\n",
    "        S3_PREFIX = f\"raw/{category.replace('.','_')}/\"\n",
    "        search = arxiv.Search(\n",
    "            query=f'cat:{category}',\n",
    "            max_results=MAX_RESULTS_GOAL,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "            sort_order=arxiv.SortOrder.Descending\n",
    "        )\n",
    "        print(f\"{category}: Started\")\n",
    "        batch = []\n",
    "        batch_ids = set()\n",
    "        pg_batch = []\n",
    "        batch_count = 0\n",
    "        total_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                results_generator = client.results(search, offset=total_count)\n",
    "                for paper_result in results_generator:\n",
    "                    entry_id = paper_result.entry_id\n",
    "                    if entry_id in existing_ids or entry_id in batch_ids:\n",
    "                        # print(f\"Skipping duplicate: {entry_id}\")\n",
    "                        continue\n",
    "                    existing_ids.add(entry_id)\n",
    "                    batch_ids.add(entry_id)\n",
    "                    paper_data = {\n",
    "                        \"entry_id\": entry_id,\n",
    "                        \"title\": paper_result.title,\n",
    "                        \"authors\": [a.name for a in paper_result.authors],\n",
    "                        \"summary\": paper_result.summary,\n",
    "                        \"primary_category\": paper_result.primary_category,\n",
    "                        \"categories\": paper_result.categories,\n",
    "                        \"published\": paper_result.published.isoformat(),\n",
    "                        \"updated\": paper_result.updated.isoformat(),\n",
    "                        \"journal_ref\": paper_result.journal_ref,\n",
    "                        \"doi\": paper_result.doi\n",
    "                    }\n",
    "                    utc_now = datetime.now(timezone.utc)\n",
    "                    today_str = utc_now.strftime(\"%Y-%m-%d\")\n",
    "                    batch.append(paper_data)\n",
    "                    total_count += 1\n",
    "                    etl_batch_id = f\"{category.replace('.','_')}_{today_str}_batch_{batch_count}\"\n",
    "                    \n",
    "                    add_to_pg_batch(entry_id, category, \"pending\", \"pending\", etl_batch_id)\n",
    "                    pg_count += 1\n",
    "                    if len(batch) >= BATCH_SIZE:\n",
    "                        # 上傳至 S3\n",
    "                        now_s3_key = upload_batch_to_s3(S3_PREFIX, batch, batch_count, category)\n",
    "                        s3_count += len(batch)\n",
    "                        for i in range(len(pg_batch)):\n",
    "                            pg_batch[i] = (pg_batch[i][0], pg_batch[i][1], \"uploaded\", pg_batch[i][3], pg_batch[i][4], pg_batch[i][5], pg_batch[i][6])\n",
    "                            \n",
    "                        # 寫入 ETL raw_batches 表\n",
    "                        add_raw_batches_to_pg(etl_batch_id, category, now_s3_key, len(batch))\n",
    "                        # 批次推送到 PG\n",
    "                        flush_pg_batch()\n",
    "                        batch = []\n",
    "                        pg_batch = []\n",
    "                        batch_count += 1\n",
    "                break\n",
    "            except arxiv.UnexpectedEmptyPageError as e:\n",
    "                logging.error(f\"Error fetching results at offset {total_count}, ignore..., detail: {e}\")\n",
    "                total_count += 1\n",
    "                continue\n",
    "        if batch:\n",
    "            now_s3_key = upload_batch_to_s3(S3_PREFIX, batch, batch_count, category)\n",
    "            s3_count += len(batch)\n",
    "            for i in range(len(pg_batch)):\n",
    "                pg_batch[i] = (pg_batch[i][0], pg_batch[i][1], \"uploaded\", pg_batch[i][3], pg_batch[i][4], pg_batch[i][5], pg_batch[i][6])\n",
    "            add_raw_batches_to_pg(etl_batch_id, category, now_s3_key, len(batch))\n",
    "            flush_pg_batch()\n",
    "            print(etl_batch_id)\n",
    "        elapsed = time.time() - start_time\n",
    "        category_stats[category] = {\"time_sec\": elapsed, \"s3_count\": s3_count, \"pg_count\": pg_count}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during category {category}: {e}\")\n",
    "    mark_category_finished(category)\n",
    "    print(f\"{category} -> Finished\")\n",
    "\n",
    "for cat, stats in category_stats.items():\n",
    "    print(f\"{cat} -> Time: {stats['time_sec']:.2f}s, S3: {stats['s3_count']}, PostgreSQL: {stats['pg_count']}\")\n",
    "\n",
    "# 批次寫入各領域統計資料\n",
    "insert_category_stats(category_stats)\n",
    "\n",
    "remaining = get_pending_categories()\n",
    "if remaining:\n",
    "    print(\"尚有領域未完成，觸發下一個 Lambda\")\n",
    "else:\n",
    "    print(\"所有領域完成，Lambda 不再觸發\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c88f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client(\n",
    "  page_size=1,\n",
    ")\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=\"\",\n",
    "    max_results=1,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    sort_order=arxiv.SortOrder.Descending,\n",
    "    id_list=[\"2301.12345v1\"]\n",
    ")\n",
    "\n",
    "result = next(client.results(search))\n",
    "for attr, value in vars(result).items():\n",
    "  print(attr,\":\",value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "input_file = Path(\"arxiv_data/arxiv_batch_1.json\")\n",
    "output_file = Path(\"arxiv_data/arxiv_batch_cleaned.json\")\n",
    "\n",
    "def transform_datetime2date(dt_str):\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(dt_str.replace(\"Z\", \"+00:00\"))\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "# 去重\n",
    "unique_papers = {paper[\"entry_id\"]: paper for paper in papers}\n",
    "\n",
    "\n",
    "required_fields = [\n",
    "    \"entry_id\", \"title\", \"summary\", \"authors\", \n",
    "    \"primary_category\", \"published\", \"updated\"\n",
    "]\n",
    "\n",
    "cleaned_papers = []\n",
    "for paper in unique_papers.values():\n",
    "    # 刪除缺值資料\n",
    "    if all(paper.get(field) for field in required_fields) and all(a.strip() for a in paper[\"authors\"]):\n",
    "        paper[\"published_date\"] = transform_datetime2date(paper[\"published\"])\n",
    "        paper[\"updated_date\"] = transform_datetime2date(paper[\"updated\"])\n",
    "        paper[\"etl_datetime\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\") # use UTC timezone\n",
    "        cleaned_papers.append(paper)\n",
    "\n",
    "for paper in cleaned_papers:\n",
    "    print(paper)\n",
    "\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(cleaned_papers, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(f\"清理完成，共 {len(cleaned_papers)} 筆，已儲存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ce2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# 建立 DynamoDB 連線\n",
    "dynamodb = boto3.resource(\n",
    "    \"dynamodb\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "table = dynamodb.Table('download_paper_entry_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新增一筆資料\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "entry_id = \"http://arxiv.org/abs/2510.11683v1\"\n",
    "item = {\n",
    "    \"category\": \"cs.LG\",\n",
    "    \"entry_id\": entry_id,\n",
    "    \"status\": \"uploaded\",  # \"failed\"\n",
    "    \"last_attempt\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"error_msg\": \"\" \n",
    "}\n",
    "\n",
    "try:\n",
    "    table.put_item(\n",
    "        Item=item,\n",
    "        ConditionExpression='attribute_not_exists(entry_id)'\n",
    "    )\n",
    "    print(\"已新增\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ConditionalCheckFailedException':\n",
    "        print(\"這篇 paper 已存在\")\n",
    "    else:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60adb45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key 查詢\n",
    "response = table.get_item(Key={\"category\": \"cs.LG\",'entry_id': entry_id})\n",
    "item = response.get('Item')\n",
    "\n",
    "if item:\n",
    "    print(\"已存在\", item)\n",
    "else:\n",
    "    print(\"不存在\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2961df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除 key\n",
    "response = table.delete_item(\n",
    "    Key={\n",
    "        \"category\": \"cs.LG\",\n",
    "        \"entry_id\": entry_id\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"刪除成功:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "def create_s3_bucket_and_prefix(bucket_name: str, domain: str):\n",
    "    env_path = os.path.join(os.path.dirname(__file__), \"../.env\")\n",
    "    if not os.path.exists(env_path):\n",
    "        raise FileNotFoundError(f\".env not found at {env_path}\")\n",
    "    \n",
    "    load_dotenv(env_path)\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        region_name=os.getenv(\"AWS_REGION\"),\n",
    "        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    )\n",
    "\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "    prefix = f\"raw/domain={domain}/\"\n",
    "    s3.put_object(Bucket=bucket_name, Key=(prefix + \".keep\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_s3_bucket_and_prefix(\"my-test-bucket\", \"cs.LG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看你有哪個 Bucket\n",
    "\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "response = s3.list_buckets()\n",
    "for bucket in response[\"Buckets\"]:\n",
    "    print(bucket[\"Name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ee619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"arvix-paper-bucket\"\n",
    "\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=\"raw/\",   # 只看 raw/ 底下\n",
    "    Delimiter=\"/\"\n",
    ")\n",
    "\n",
    "if \"CommonPrefixes\" in response:\n",
    "    print(\"Prefixes:\")\n",
    "    for prefix in response[\"CommonPrefixes\"]:\n",
    "        print(prefix[\"Prefix\"])\n",
    "else:\n",
    "    print(\"沒有找到任何 prefix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "101f67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上傳 config.yaml 至 S3\n",
    "\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"arvix-paper-bucket\"\n",
    "local_file = \"../config/config.yaml\"\n",
    "key = \"config/config.yaml\"\n",
    "\n",
    "with open(local_file, \"rb\") as f:\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=f,\n",
    "        ContentType=\"application/x-yaml\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 S3 上的 config.yaml\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"arvix-paper-bucket\"\n",
    "local_path = \"/tmp/config.yaml\"\n",
    "key = \"config/config.yaml\"\n",
    "s3.download_file(bucket_name, key, local_path)\n",
    "\n",
    "with open(local_path) as f:\n",
    "    data = f.read()\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68657f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "prefix = \"raw/domain=cs.LG/\"\n",
    "local_file = \"/home/hank/hackmd-data-pipeline/tests/arxiv_data/arxiv_batch_2.json\"\n",
    "key = prefix + os.path.basename(local_file)\n",
    "\n",
    "with open(local_file, \"rb\") as f:\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=f,\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "print(f\"已上傳 {local_file} 到 S3: {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "prefix = \"raw/domain=cs.LG/\"\n",
    "\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=prefix,\n",
    "    Delimiter=\"/\" \n",
    ")\n",
    "\n",
    "if \"Contents\" in response:\n",
    "    print(\"檔案列表：\")\n",
    "    files = [obj[\"Key\"] for obj in response[\"Contents\"] if not obj[\"Key\"].endswith(\".keep\")]\n",
    "    for f in files:\n",
    "        print(f)\n",
    "else:\n",
    "    print(\"此 prefix 下沒有檔案\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.db import get_pg\n",
    "from src.core.pg_engine import PsqlEngine\n",
    "pg = get_pg()\n",
    "\n",
    "def paper_exists(pg: PsqlEngine, category: str, entry_id: str) -> bool:\n",
    "    stmt = f\"\"\"\n",
    "        SELECT 1\n",
    "        FROM papers.downloaded_papers\n",
    "        WHERE category = '{category}' AND entry_id = '{entry_id}'\n",
    "        LIMIT 1;\n",
    "    \"\"\"\n",
    "    result = pg.execute_query(stmt)\n",
    "    return bool(result)\n",
    "\n",
    "\n",
    "paper_exists(pg, \"cs_LG\",'dsfd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackmd-data-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
