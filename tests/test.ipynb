{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c23e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取環境變數\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "sys.path.append(str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37300cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client(\n",
    "  page_size=1,\n",
    ")\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=\"\",\n",
    "    max_results=1,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    sort_order=arxiv.SortOrder.Descending,\n",
    "    id_list=[\"2301.12345v1\"]\n",
    ")\n",
    "\n",
    "result = next(client.results(search))\n",
    "for attr, value in vars(result).items():\n",
    "  print(attr,\":\",value)\n",
    "    \n",
    "# print(\"ID:\", result.entry_id)\n",
    "# print(\"標題:\", result.title)\n",
    "# print(\"摘要:\", result.summary)\n",
    "# print(\"作者:\", \", \".join(str(a) for a in result.authors))\n",
    "# print(\"主分類:\", result.primary_category)\n",
    "# print(\"其他分類:\", \", \".join(result.categories))\n",
    "# print(\"發表日期:\", result.published)\n",
    "# print(\"更新日期:\", result.updated)\n",
    "# print(\"期刊/會議資訊:\", result.journal_ref)\n",
    "# print(\"DOI:\", result.doi)\n",
    "# print(\"PDF_url:\", result.pdf_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a094a614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs.AI: Started\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 168\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    167\u001b[39m     results_generator = client.results(search, offset=total_count)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaper_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults_generator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtotal_count\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzero\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/.venv/lib/python3.12/site-packages/arxiv/__init__.py:601\u001b[39m, in \u001b[36mClient._results\u001b[39m\u001b[34m(self, search, offset)\u001b[39m\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, search: Search, offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m) -> Generator[Result, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m    600\u001b[39m     page_url = \u001b[38;5;28mself\u001b[39m._format_url(search, offset, \u001b[38;5;28mself\u001b[39m.page_size)\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     feed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m feed.entries:\n\u001b[32m    603\u001b[39m         logger.info(\u001b[33m\"\u001b[39m\u001b[33mGot empty first page; stopping generation\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/.venv/lib/python3.12/site-packages/arxiv/__init__.py:648\u001b[39m, in \u001b[36mClient._parse_feed\u001b[39m\u001b[34m(self, url, first_page, _try_index)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[33;03mFetches the specified URL and parses it with feedparser.\u001b[39;00m\n\u001b[32m    643\u001b[39m \n\u001b[32m    644\u001b[39m \u001b[33;03mIf a request fails or is unexpectedly empty, retries the request up to\u001b[39;00m\n\u001b[32m    645\u001b[39m \u001b[33;03m`self.num_retries` times.\u001b[39;00m\n\u001b[32m    646\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    650\u001b[39m     HTTPError,\n\u001b[32m    651\u001b[39m     UnexpectedEmptyPageError,\n\u001b[32m    652\u001b[39m     requests.exceptions.ConnectionError,\n\u001b[32m    653\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    654\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _try_index < \u001b[38;5;28mself\u001b[39m.num_retries:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/.venv/lib/python3.12/site-packages/arxiv/__init__.py:682\u001b[39m, in \u001b[36mClient.__try_parse_feed\u001b[39m\u001b[34m(self, url, first_page, try_index)\u001b[39m\n\u001b[32m    678\u001b[39m         time.sleep(to_sleep)\n\u001b[32m    680\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mRequesting page (first: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m, try: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, first_page, try_index, url)\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser-agent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marxiv.py/2.2.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28mself\u001b[39m._last_request_dt = datetime.now()\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resp.status_code != requests.codes.OK:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/.venv/lib/python3.12/site-packages/requests/sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/.venv/lib/python3.12/site-packages/requests/sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/.venv/lib/python3.12/site-packages/requests/models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/.venv/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/.venv/lib/python3.12/site-packages/urllib3/response.py:1060\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1059\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1062\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1063\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/.venv/lib/python3.12/site-packages/urllib3/response.py:949\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    946\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    947\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/.venv/lib/python3.12/site-packages/urllib3/response.py:873\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    870\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    874\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    875\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    876\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    882\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    883\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/.venv/lib/python3.12/site-packages/urllib3/response.py:856\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    853\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    854\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    855\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/http/client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import time\n",
    "import time\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import gzip\n",
    "import io\n",
    "import boto3\n",
    "from datetime import datetime, timezone\n",
    "from src.core.db import get_pg\n",
    "from src.core.pg_engine import PsqlEngine\n",
    "\n",
    "pg = get_pg()\n",
    "load_dotenv(\"../.env\")\n",
    "logging.getLogger(\"arxiv\").setLevel(logging.WARNING)\n",
    "\n",
    "category_list = [\n",
    "    # Computer Science\n",
    "    \"cs.AI\", \"cs.LG\", \"cs.CV\", \"cs.CL\", \"cs.NE\", \"cs.RO\", \"cs.HC\", \"cs.SE\", \"cs.DS\",\n",
    "    \"cs.DB\", \"cs.SY\", \"cs.OS\", \"cs.PF\", \"cs.PL\", \"cs.MS\", \"cs.CC\", \"cs.CG\", \"cs.LO\",\n",
    "    \"cs.GT\", \"cs.MM\", \"cs.IT\",\n",
    "\n",
    "    # Mathematics\n",
    "    \"math.CO\", \"math.AG\", \"math.GT\", \"math.QA\", \"math.RA\", \"math.NT\", \"math.KT\", \"math.DG\",\n",
    "    \"math.DS\", \"math.FA\", \"math.AP\", \"math.SP\", \"math.ST\", \"math.PR\",\n",
    "\n",
    "    # Statistics\n",
    "    \"stat.ML\", \"stat.TH\", \"stat.CO\", \"stat.AP\", \"stat.OT\", \"stat.ME\",\n",
    "\n",
    "    # Physics\n",
    "    \"physics.optics\", \"physics.bio-ph\", \"physics.gen-ph\", \"physics.acc-ph\", \"physics.chem-ph\",\n",
    "    \"physics.class-ph\", \"physics.comp-ph\", \"physics.data-an\", \"physics.ed-ph\", \"physics.ins-det\",\n",
    "    \"physics.med-ph\", \"physics.plasm-ph\", \"physics.space-ph\",\n",
    "\n",
    "    # Quantitative Biology\n",
    "    \"q-bio.BM\", \"q-bio.CB\", \"q-bio.GN\", \"q-bio.MN\", \"q-bio.NC\", \"q-bio.PE\", \"q-bio.QM\", \"q-bio.SC\",\n",
    "\n",
    "    # Quantitative Finance\n",
    "    \"q-fin.CP\", \"q-fin.EC\", \"q-fin.GN\", \"q-fin.MF\", \"q-fin.PM\", \"q-fin.RM\", \"q-fin.ST\", \"q-fin.TR\",\n",
    "\n",
    "    # Electrical Engineering and Systems Science\n",
    "    \"eess.AS\", \"eess.IV\", \"eess.SP\", \"eess.SY\",\n",
    "\n",
    "    # Astrophysics\n",
    "    \"astro-ph.CO\", \"astro-ph.GA\", \"astro-ph.HE\", \"astro-ph.IM\", \"astro-ph.SR\",\n",
    "\n",
    "    # Condensed Matter Physics\n",
    "    \"cond-mat.mtrl-sci\", \"cond-mat.str-el\", \"cond-mat.supr-con\", \"cond-mat.quant-gas\",\n",
    "    \"cond-mat.dis-nn\", \"cond-mat.soft\", \"cond-mat.stat-mech\",\n",
    "\n",
    "    # High Energy Physics\n",
    "    \"hep-ex\", \"hep-lat\", \"hep-ph\", \"hep-th\"\n",
    "]\n",
    "\n",
    "category_list = [\"cs.AI\"]\n",
    "\n",
    "MAX_RESULTS_GOAL = 1000\n",
    "BATCH_SIZE = 100\n",
    "S3_BUCKET = \"hackmd-paper-bucket\"\n",
    "MAX_ATTEMPTS = 3\n",
    "INITIAL_DELAY_SECONDS = 5\n",
    "\n",
    "client = arxiv.Client(\n",
    "    page_size=MAX_RESULTS_GOAL, \n",
    "    delay_seconds=3,\n",
    "    num_retries=3\n",
    ")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "def load_existing_ids():\n",
    "    stmt = f\"\"\"\n",
    "        SELECT entry_id\n",
    "        FROM papers.downloaded_papers\n",
    "    \"\"\"\n",
    "    rows = pg.execute_query(stmt)\n",
    "    return set(r[0] for r in rows)\n",
    "\n",
    "def add_to_pg_batch(entry_id, category, status, etl_status, etl_batch_id=None, error_msg=\"\"):\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    pg_batch.append((entry_id, category, status, now_utc, error_msg, etl_status, etl_batch_id))\n",
    "\n",
    "\n",
    "def flush_pg_batch():\n",
    "    global pg_batch\n",
    "    if not pg_batch:\n",
    "        return\n",
    "    try:\n",
    "        pg.insert_mogrify(\"papers.downloaded_papers\", pg_batch)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to insert batch into Postgres: {e}\")\n",
    "    finally:\n",
    "        pg_batch = []\n",
    "        \n",
    "def add_raw_batches_to_pg(batch_id, category, s3_path, record_count, ):\n",
    "    stmt = \"\"\"\n",
    "        INSERT INTO etl.raw_batches (batch_id, category, s3_path, record_count)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "        ON CONFLICT (batch_id) DO NOTHING;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pg.execute_cmd(stmt, (batch_id, category, s3_path, record_count))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to insert into raw_batches: {e}\")\n",
    "\n",
    "def upload_batch_to_s3(s3_prefix, batch_data, batch_num, category):\n",
    "    if not batch_data:\n",
    "        return\n",
    "    jsonl_content = \"\\n\".join([json.dumps(paper, ensure_ascii=False) for paper in batch_data])\n",
    "    buffer = io.BytesIO()\n",
    "    with gzip.GzipFile(fileobj=buffer, mode='wb') as f:\n",
    "        f.write(jsonl_content.encode('utf-8'))\n",
    "    gzip_bytes = buffer.getvalue()\n",
    "    \n",
    "    utc_now = datetime.now(timezone.utc)\n",
    "    today_str = utc_now.strftime(\"%Y-%m-%d\")\n",
    "    utc_timestamp = int(utc_now.timestamp())\n",
    "    s3_key = f\"{s3_prefix}{today_str}/{category.replace('.','_')}_batch_{batch_num}_{utc_timestamp}.jsonl.gz\"\n",
    "    \n",
    "    last_exception = None\n",
    "    for attempt in range(MAX_ATTEMPTS):\n",
    "        try:\n",
    "            s3.put_object(\n",
    "                Bucket=S3_BUCKET,\n",
    "                Key=s3_key,\n",
    "                Body=gzip_bytes,\n",
    "                ContentType='application/jsonl'\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_exception = e\n",
    "            if attempt < MAX_ATTEMPTS - 1:\n",
    "                time.sleep(INITIAL_DELAY_SECONDS * (2 ** attempt))\n",
    "            else:\n",
    "                raise last_exception\n",
    "    return s3_key\n",
    "\n",
    "existing_ids = load_existing_ids()\n",
    "category_stats = {}\n",
    "for category in category_list:\n",
    "    start_time = time.time()\n",
    "    s3_count = 0\n",
    "    pg_count = 0\n",
    "    try:\n",
    "        S3_PREFIX = f\"raw/{category.replace('.','_')}/\"\n",
    "        search = arxiv.Search(\n",
    "            query=f'cat:{category}',\n",
    "            max_results=MAX_RESULTS_GOAL,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "            sort_order=arxiv.SortOrder.Descending\n",
    "        )\n",
    "        print(f\"{category}: Started\")\n",
    "        batch = []\n",
    "        batch_ids = set()\n",
    "        pg_batch = []\n",
    "        batch_count = 0\n",
    "        total_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                results_generator = client.results(search, offset=total_count)\n",
    "                for paper_result in results_generator:\n",
    "                    if total_count == 0:\n",
    "                        print(f\"zero\")\n",
    "                    entry_id = paper_result.entry_id\n",
    "                    if entry_id in existing_ids or entry_id in batch_ids:\n",
    "                        print(f\"Skipping duplicate: {entry_id}\")\n",
    "                        continue\n",
    "                    existing_ids.add(entry_id)\n",
    "                    batch_ids.add(entry_id)\n",
    "                    paper_data = {\n",
    "                        \"entry_id\": entry_id,\n",
    "                        \"title\": paper_result.title,\n",
    "                        \"authors\": [a.name for a in paper_result.authors],\n",
    "                        \"summary\": paper_result.summary,\n",
    "                        \"primary_category\": paper_result.primary_category,\n",
    "                        \"categories\": paper_result.categories,\n",
    "                        \"published\": paper_result.published.isoformat(),\n",
    "                        \"updated\": paper_result.updated.isoformat(),\n",
    "                        \"journal_ref\": paper_result.journal_ref,\n",
    "                        \"doi\": paper_result.doi\n",
    "                    }\n",
    "                    utc_now = datetime.now(timezone.utc)\n",
    "                    today_str = utc_now.strftime(\"%Y-%m-%d\")\n",
    "                    batch.append(paper_data)\n",
    "                    total_count += 1\n",
    "                    etl_batch_id = f\"{category.replace('.','_')}_{today_str}_batch_{batch_count}\"\n",
    "                    \n",
    "                    # entry_id, category, status, etl_status, etl_batch_id=None, error_msg=\"\"   \n",
    "                    add_to_pg_batch(entry_id, category, \"pending\", \"pending\", etl_batch_id)\n",
    "                    pg_count += 1\n",
    "                    if len(batch) >= BATCH_SIZE:\n",
    "                        print(pg_count)\n",
    "                        # 上傳至 S3\n",
    "                        now_s3_key = upload_batch_to_s3(S3_PREFIX, batch, batch_count, category)\n",
    "                        s3_count += len(batch)\n",
    "                        for i in range(len(pg_batch)):\n",
    "                            pg_batch[i] = (pg_batch[i][0], pg_batch[i][1], \"uploaded\", pg_batch[i][3], pg_batch[i][4])\n",
    "                            print(pg_batch[i])\n",
    "                        # 寫入 ETL raw_batches 表\n",
    "                        add_raw_batches_to_pg(etl_batch_id, category, now_s3_key, len(batch))\n",
    "                        # 批次推送到 PG\n",
    "                        flush_pg_batch()\n",
    "                        print(etl_batch_id)\n",
    "                        batch = []\n",
    "                        pg_batch = []\n",
    "                        batch_count += 1\n",
    "                break\n",
    "            except arxiv.UnexpectedEmptyPageError:\n",
    "                total_count += 1\n",
    "                continue\n",
    "        if batch:\n",
    "            upload_batch_to_s3(S3_PREFIX, batch, batch_count, category)\n",
    "            s3_count += len(batch)\n",
    "            for i in range(len(pg_batch)):\n",
    "                pg_batch[i] = (pg_batch[i][0], pg_batch[i][1], \"uploaded\", pg_batch[i][3], pg_batch[i][4], pg_batch[i][5])\n",
    "            flush_pg_batch()\n",
    "        elapsed = time.time() - start_time\n",
    "        category_stats[category] = {\"time_sec\": elapsed, \"s3_count\": s3_count, \"pg_count\": pg_count}\n",
    "        print(f\"{category}: Finished\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during category {category}: {e}\")\n",
    "\n",
    "for cat, stats in category_stats.items():\n",
    "    print(f\"{cat} -> Time: {stats['time_sec']:.2f}s, S3: {stats['s3_count']}, PostgreSQL: {stats['pg_count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "input_file = Path(\"arxiv_data/arxiv_batch_1.json\")\n",
    "output_file = Path(\"arxiv_data/arxiv_batch_cleaned.json\")\n",
    "\n",
    "def transform_datetime2date(dt_str):\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(dt_str.replace(\"Z\", \"+00:00\"))\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "# 去重\n",
    "unique_papers = {paper[\"entry_id\"]: paper for paper in papers}\n",
    "\n",
    "\n",
    "required_fields = [\n",
    "    \"entry_id\", \"title\", \"summary\", \"authors\", \n",
    "    \"primary_category\", \"published\", \"updated\"\n",
    "]\n",
    "\n",
    "cleaned_papers = []\n",
    "for paper in unique_papers.values():\n",
    "    # 刪除缺值資料\n",
    "    if all(paper.get(field) for field in required_fields) and all(a.strip() for a in paper[\"authors\"]):\n",
    "        paper[\"published_date\"] = transform_datetime2date(paper[\"published\"])\n",
    "        paper[\"updated_date\"] = transform_datetime2date(paper[\"updated\"])\n",
    "        paper[\"etl_datetime\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\") # use UTC timezone\n",
    "        cleaned_papers.append(paper)\n",
    "\n",
    "for paper in cleaned_papers:\n",
    "    print(paper)\n",
    "\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(cleaned_papers, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(f\"清理完成，共 {len(cleaned_papers)} 筆，已儲存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ce2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# 建立 DynamoDB 連線\n",
    "dynamodb = boto3.resource(\n",
    "    \"dynamodb\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "table = dynamodb.Table('download_paper_entry_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新增一筆資料\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "entry_id = \"http://arxiv.org/abs/2510.11683v1\"\n",
    "item = {\n",
    "    \"category\": \"cs.LG\",\n",
    "    \"entry_id\": entry_id,\n",
    "    \"status\": \"uploaded\",  # \"failed\"\n",
    "    \"last_attempt\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"error_msg\": \"\" \n",
    "}\n",
    "\n",
    "try:\n",
    "    table.put_item(\n",
    "        Item=item,\n",
    "        ConditionExpression='attribute_not_exists(entry_id)'\n",
    "    )\n",
    "    print(\"已新增\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ConditionalCheckFailedException':\n",
    "        print(\"這篇 paper 已存在\")\n",
    "    else:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60adb45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key 查詢\n",
    "response = table.get_item(Key={\"category\": \"cs.LG\",'entry_id': entry_id})\n",
    "item = response.get('Item')\n",
    "\n",
    "if item:\n",
    "    print(\"已存在\", item)\n",
    "else:\n",
    "    print(\"不存在\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2961df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除 key\n",
    "response = table.delete_item(\n",
    "    Key={\n",
    "        \"category\": \"cs.LG\",\n",
    "        \"entry_id\": entry_id\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"刪除成功:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "def create_s3_bucket_and_prefix(bucket_name: str, domain: str):\n",
    "    env_path = os.path.join(os.path.dirname(__file__), \"../.env\")\n",
    "    if not os.path.exists(env_path):\n",
    "        raise FileNotFoundError(f\".env not found at {env_path}\")\n",
    "    \n",
    "    load_dotenv(env_path)\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        region_name=os.getenv(\"AWS_REGION\"),\n",
    "        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    )\n",
    "\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "    prefix = f\"raw/domain={domain}/\"\n",
    "    s3.put_object(Bucket=bucket_name, Key=(prefix + \".keep\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_s3_bucket_and_prefix(\"my-test-bucket\", \"cs.LG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看你有哪個 Bucket\n",
    "\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "response = s3.list_buckets()\n",
    "for bucket in response[\"Buckets\"]:\n",
    "    print(bucket[\"Name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ee619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=\"raw/\",   # 只看 raw/ 底下\n",
    "    Delimiter=\"/\"\n",
    ")\n",
    "\n",
    "if \"CommonPrefixes\" in response:\n",
    "    print(\"Prefixes:\")\n",
    "    for prefix in response[\"CommonPrefixes\"]:\n",
    "        print(prefix[\"Prefix\"])\n",
    "else:\n",
    "    print(\"沒有找到任何 prefix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68657f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "prefix = \"raw/domain=cs.LG/\"\n",
    "local_file = \"/home/hank/hackmd-data-pipeline/tests/arxiv_data/arxiv_batch_2.json\"\n",
    "key = prefix + os.path.basename(local_file)\n",
    "\n",
    "with open(local_file, \"rb\") as f:\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=f,\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "print(f\"已上傳 {local_file} 到 S3: {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "prefix = \"raw/domain=cs.LG/\"\n",
    "\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=prefix,\n",
    "    Delimiter=\"/\" \n",
    ")\n",
    "\n",
    "if \"Contents\" in response:\n",
    "    print(\"檔案列表：\")\n",
    "    files = [obj[\"Key\"] for obj in response[\"Contents\"] if not obj[\"Key\"].endswith(\".keep\")]\n",
    "    for f in files:\n",
    "        print(f)\n",
    "else:\n",
    "    print(\"此 prefix 下沒有檔案\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.db import get_pg\n",
    "from src.core.pg_engine import PsqlEngine\n",
    "pg = get_pg()\n",
    "\n",
    "def paper_exists(pg: PsqlEngine, category: str, entry_id: str) -> bool:\n",
    "    stmt = f\"\"\"\n",
    "        SELECT 1\n",
    "        FROM papers.downloaded_papers\n",
    "        WHERE category = '{category}' AND entry_id = '{entry_id}'\n",
    "        LIMIT 1;\n",
    "    \"\"\"\n",
    "    result = pg.execute_query(stmt)\n",
    "    return bool(result)\n",
    "\n",
    "\n",
    "paper_exists(pg, \"cs_LG\",'dsfd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackmd-data-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
