{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37300cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client(\n",
    "  page_size=1,\n",
    ")\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=\"\",\n",
    "    max_results=1,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    sort_order=arxiv.SortOrder.Descending,\n",
    "    id_list=[\"2301.12345v1\"]\n",
    ")\n",
    "\n",
    "result = next(client.results(search))\n",
    "for attr, value in vars(result).items():\n",
    "  print(attr,\":\",value)\n",
    "    \n",
    "# print(\"ID:\", result.entry_id)\n",
    "# print(\"標題:\", result.title)\n",
    "# print(\"摘要:\", result.summary)\n",
    "# print(\"作者:\", \", \".join(str(a) for a in result.authors))\n",
    "# print(\"主分類:\", result.primary_category)\n",
    "# print(\"其他分類:\", \", \".join(result.categories))\n",
    "# print(\"發表日期:\", result.published)\n",
    "# print(\"更新日期:\", result.updated)\n",
    "# print(\"期刊/會議資訊:\", result.journal_ref)\n",
    "# print(\"DOI:\", result.doi)\n",
    "# print(\"PDF_url:\", result.pdf_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a094a614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded batch 0 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_DS/2025-10-15/cs_DS_batch_0_1760547760.jsonl\n",
      "Successfully uploaded batch 1 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_DS/2025-10-15/cs_DS_batch_1_1760547761.jsonl\n",
      "Successfully uploaded batch 2 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_DS/2025-10-15/cs_DS_batch_2_1760547762.jsonl\n",
      "Successfully uploaded batch 3 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_DS/2025-10-15/cs_DS_batch_3_1760547762.jsonl\n",
      "Successfully uploaded batch 4 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_DS/2025-10-15/cs_DS_batch_4_1760547763.jsonl\n",
      "Successfully uploaded batch 5 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_DS/2025-10-15/cs_DS_batch_5_1760547763.jsonl\n",
      "Successfully uploaded batch 6 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_DS/2025-10-15/cs_DS_batch_6_1760547763.jsonl\n",
      "Successfully uploaded batch 7 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_DS/2025-10-15/cs_DS_batch_7_1760547764.jsonl\n",
      "Successfully uploaded batch 8 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_DS/2025-10-15/cs_DS_batch_8_1760547765.jsonl\n",
      "Successfully uploaded batch 9 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_DS/2025-10-15/cs_DS_batch_9_1760547765.jsonl\n",
      "Successfully uploaded batch 0 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_AI/2025-10-15/cs_AI_batch_0_1760547769.jsonl\n",
      "Successfully uploaded batch 1 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_AI/2025-10-15/cs_AI_batch_1_1760547770.jsonl\n",
      "Successfully uploaded batch 2 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_AI/2025-10-15/cs_AI_batch_2_1760547771.jsonl\n",
      "Successfully uploaded batch 3 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_AI/2025-10-15/cs_AI_batch_3_1760547771.jsonl\n",
      "Successfully uploaded batch 4 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_AI/2025-10-15/cs_AI_batch_4_1760547772.jsonl\n",
      "Successfully uploaded batch 5 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_AI/2025-10-15/cs_AI_batch_5_1760547772.jsonl\n",
      "Successfully uploaded batch 6 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_AI/2025-10-15/cs_AI_batch_6_1760547774.jsonl\n",
      "Successfully uploaded batch 7 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_AI/2025-10-15/cs_AI_batch_7_1760547775.jsonl\n",
      "Successfully uploaded batch 8 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_AI/2025-10-15/cs_AI_batch_8_1760547775.jsonl\n",
      "Successfully uploaded batch 9 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_AI/2025-10-15/cs_AI_batch_9_1760547776.jsonl\n",
      "Successfully uploaded batch 0 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_LG/2025-10-15/cs_LG_batch_0_1760547777.jsonl\n",
      "Successfully uploaded batch 1 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_LG/2025-10-15/cs_LG_batch_1_1760547784.jsonl\n",
      "Successfully uploaded batch 2 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_LG/2025-10-15/cs_LG_batch_2_1760547786.jsonl\n",
      "Successfully uploaded batch 3 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_LG/2025-10-15/cs_LG_batch_3_1760547787.jsonl\n",
      "Successfully uploaded batch 4 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_LG/2025-10-15/cs_LG_batch_4_1760547787.jsonl\n",
      "Successfully uploaded batch 5 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_LG/2025-10-15/cs_LG_batch_5_1760547788.jsonl\n",
      "Successfully uploaded batch 6 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_LG/2025-10-15/cs_LG_batch_6_1760547788.jsonl\n",
      "Successfully uploaded batch 7 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_LG/2025-10-15/cs_LG_batch_7_1760547789.jsonl\n",
      "Successfully uploaded batch 8 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_LG/2025-10-15/cs_LG_batch_8_1760547789.jsonl\n",
      "Successfully uploaded batch 9 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_LG/2025-10-15/cs_LG_batch_9_1760547789.jsonl\n",
      "Successfully uploaded batch 0 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CV/2025-10-15/cs_CV_batch_0_1760547794.jsonl\n",
      "Successfully uploaded batch 1 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CV/2025-10-15/cs_CV_batch_1_1760547795.jsonl\n",
      "Successfully uploaded batch 2 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CV/2025-10-15/cs_CV_batch_2_1760547795.jsonl\n",
      "Successfully uploaded batch 3 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CV/2025-10-15/cs_CV_batch_3_1760547795.jsonl\n",
      "Successfully uploaded batch 4 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CV/2025-10-15/cs_CV_batch_4_1760547796.jsonl\n",
      "Successfully uploaded batch 5 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CV/2025-10-15/cs_CV_batch_5_1760547796.jsonl\n",
      "Successfully uploaded batch 6 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CV/2025-10-15/cs_CV_batch_6_1760547797.jsonl\n",
      "Successfully uploaded batch 7 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CV/2025-10-15/cs_CV_batch_7_1760547797.jsonl\n",
      "Successfully uploaded batch 8 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CV/2025-10-15/cs_CV_batch_8_1760547798.jsonl\n",
      "Successfully uploaded batch 9 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CV/2025-10-15/cs_CV_batch_9_1760547798.jsonl\n",
      "Successfully uploaded batch 0 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CL/2025-10-15/cs_CL_batch_0_1760547802.jsonl\n",
      "Successfully uploaded batch 1 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CL/2025-10-15/cs_CL_batch_1_1760547803.jsonl\n",
      "Successfully uploaded batch 2 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CL/2025-10-15/cs_CL_batch_2_1760547804.jsonl\n",
      "Successfully uploaded batch 3 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CL/2025-10-15/cs_CL_batch_3_1760547804.jsonl\n",
      "Successfully uploaded batch 4 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CL/2025-10-15/cs_CL_batch_4_1760547805.jsonl\n",
      "Successfully uploaded batch 5 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CL/2025-10-15/cs_CL_batch_5_1760547805.jsonl\n",
      "Successfully uploaded batch 6 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CL/2025-10-15/cs_CL_batch_6_1760547805.jsonl\n",
      "Successfully uploaded batch 7 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CL/2025-10-15/cs_CL_batch_7_1760547806.jsonl\n",
      "Successfully uploaded batch 8 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CL/2025-10-15/cs_CL_batch_8_1760547806.jsonl\n",
      "Successfully uploaded batch 9 (100 papers) to s3://hackmd-paper-bucket/raw/domain=cs_CL/2025-10-15/cs_CL_batch_9_1760547808.jsonl\n",
      "Successfully uploaded batch 0 (100 papers) to s3://hackmd-paper-bucket/raw/domain=stat_ML/2025-10-15/stat_ML_batch_0_1760547810.jsonl\n",
      "Successfully uploaded batch 1 (100 papers) to s3://hackmd-paper-bucket/raw/domain=stat_ML/2025-10-15/stat_ML_batch_1_1760547810.jsonl\n",
      "Successfully uploaded batch 2 (100 papers) to s3://hackmd-paper-bucket/raw/domain=stat_ML/2025-10-15/stat_ML_batch_2_1760547817.jsonl\n",
      "Successfully uploaded batch 3 (100 papers) to s3://hackmd-paper-bucket/raw/domain=stat_ML/2025-10-15/stat_ML_batch_3_1760547819.jsonl\n",
      "Successfully uploaded batch 4 (100 papers) to s3://hackmd-paper-bucket/raw/domain=stat_ML/2025-10-15/stat_ML_batch_4_1760547820.jsonl\n",
      "Successfully uploaded batch 5 (100 papers) to s3://hackmd-paper-bucket/raw/domain=stat_ML/2025-10-15/stat_ML_batch_5_1760547821.jsonl\n",
      "Successfully uploaded batch 6 (100 papers) to s3://hackmd-paper-bucket/raw/domain=stat_ML/2025-10-15/stat_ML_batch_6_1760547822.jsonl\n",
      "Successfully uploaded batch 7 (100 papers) to s3://hackmd-paper-bucket/raw/domain=stat_ML/2025-10-15/stat_ML_batch_7_1760547823.jsonl\n",
      "Successfully uploaded batch 8 (100 papers) to s3://hackmd-paper-bucket/raw/domain=stat_ML/2025-10-15/stat_ML_batch_8_1760547824.jsonl\n",
      "Successfully uploaded batch 9 (100 papers) to s3://hackmd-paper-bucket/raw/domain=stat_ML/2025-10-15/stat_ML_batch_9_1760547825.jsonl\n",
      "Successfully uploaded batch 0 (100 papers) to s3://hackmd-paper-bucket/raw/domain=math_ST/2025-10-15/math_ST_batch_0_1760547830.jsonl\n",
      "Successfully uploaded batch 1 (100 papers) to s3://hackmd-paper-bucket/raw/domain=math_ST/2025-10-15/math_ST_batch_1_1760547831.jsonl\n",
      "Successfully uploaded batch 2 (100 papers) to s3://hackmd-paper-bucket/raw/domain=math_ST/2025-10-15/math_ST_batch_2_1760547832.jsonl\n",
      "Successfully uploaded batch 3 (100 papers) to s3://hackmd-paper-bucket/raw/domain=math_ST/2025-10-15/math_ST_batch_3_1760547833.jsonl\n",
      "Successfully uploaded batch 4 (100 papers) to s3://hackmd-paper-bucket/raw/domain=math_ST/2025-10-15/math_ST_batch_4_1760547834.jsonl\n",
      "Successfully uploaded batch 5 (100 papers) to s3://hackmd-paper-bucket/raw/domain=math_ST/2025-10-15/math_ST_batch_5_1760547835.jsonl\n",
      "Successfully uploaded batch 6 (100 papers) to s3://hackmd-paper-bucket/raw/domain=math_ST/2025-10-15/math_ST_batch_6_1760547835.jsonl\n",
      "Successfully uploaded batch 7 (100 papers) to s3://hackmd-paper-bucket/raw/domain=math_ST/2025-10-15/math_ST_batch_7_1760547837.jsonl\n",
      "Successfully uploaded batch 8 (100 papers) to s3://hackmd-paper-bucket/raw/domain=math_ST/2025-10-15/math_ST_batch_8_1760547838.jsonl\n",
      "Successfully uploaded batch 9 (100 papers) to s3://hackmd-paper-bucket/raw/domain=math_ST/2025-10-15/math_ST_batch_9_1760547839.jsonl\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import boto3\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "logging.getLogger(\"arxiv\").setLevel(logging.WARNING)\n",
    "\n",
    "category_list = [\"cs.DS\", \"cs.AI\", \"cs.LG\", \"cs.CV\", \"cs.CL\", \"stat.ML\", \"math.ST\"]\n",
    "MAX_RESULTS_GOAL = 1000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "S3_BUCKET = \"hackmd-paper-bucket\"\n",
    "\n",
    "MAX_ATTEMPTS = 3\n",
    "INITIAL_DELAY_SECONDS = 5\n",
    "\n",
    "last_exception = None\n",
    "\n",
    "client = arxiv.Client(\n",
    "    page_size=MAX_RESULTS_GOAL, \n",
    "    delay_seconds=3,\n",
    "    num_retries=3\n",
    ")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    )\n",
    "        \n",
    "def upload_batch_to_s3(s3_prefix, batch_data, batch_num):\n",
    "    if not batch_data:\n",
    "        return\n",
    "    \n",
    "    jsonl_content = \"\\n\".join([json.dumps(paper, ensure_ascii=False) for paper in batch_data])\n",
    "    \n",
    "    utc_now = datetime.now(timezone.utc)\n",
    "    today_str = utc_now.strftime(\"%Y-%m-%d\")\n",
    "    utc_timestamp = int(utc_now.timestamp())\n",
    "    s3_key = f\"{s3_prefix}{today_str}/{category.replace('.','_')}_batch_{batch_num}_{utc_timestamp}.jsonl\"\n",
    "    \n",
    "    for attempt in range(MAX_ATTEMPTS):\n",
    "        try:\n",
    "            s3.put_object(\n",
    "                Bucket=S3_BUCKET,\n",
    "                Key=s3_key,\n",
    "                Body=jsonl_content.encode('utf-8'),\n",
    "                ContentType='application/jsonl'\n",
    "            )\n",
    "            print(f\"Successfully uploaded batch {batch_num} ({len(batch_data)} papers) to s3://{S3_BUCKET}/{s3_key}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_exception = e\n",
    "            print(f\"Attempt {attempt + 1}/{MAX_ATTEMPTS} failed for batch {batch_num}. Error: {e}\")\n",
    "            if attempt < MAX_ATTEMPTS - 1:\n",
    "                delay = INITIAL_DELAY_SECONDS * (2 ** attempt)\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"All {MAX_ATTEMPTS} attempts failed for batch {batch_num}. Could not upload.\")\n",
    "                raise last_exception\n",
    "\n",
    "try:\n",
    "    for category in category_list:\n",
    "        S3_PREFIX = f\"raw/domain={category.replace('.','_')}/\"\n",
    "        \n",
    "        search = arxiv.Search(\n",
    "            query=f'cat:{category}',\n",
    "            max_results=MAX_RESULTS_GOAL,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "            sort_order=arxiv.SortOrder.Descending\n",
    "        )\n",
    "        \n",
    "        results_generator = client.results(search)\n",
    "        \n",
    "        batch = []\n",
    "        batch_count = 0\n",
    "        \n",
    "        for paper_result in results_generator:\n",
    "            paper_data = {\n",
    "                \"entry_id\": paper_result.entry_id,\n",
    "                \"title\": paper_result.title,\n",
    "                \"authors\": [a.name for a in paper_result.authors],\n",
    "                \"summary\": paper_result.summary,\n",
    "                \"primary_category\": paper_result.primary_category,\n",
    "                \"categories\": paper_result.categories,\n",
    "                \"published\": paper_result.published.isoformat(),\n",
    "                \"updated\": paper_result.updated.isoformat(),\n",
    "                \"journal_ref\": paper_result.journal_ref,\n",
    "                \"doi\": paper_result.doi\n",
    "            }\n",
    "            batch.append(paper_data)\n",
    "            \n",
    "            if len(batch) >= BATCH_SIZE:\n",
    "                upload_batch_to_s3(S3_PREFIX, batch, batch_count)\n",
    "                batch = []\n",
    "                batch_count += 1\n",
    "                \n",
    "        if batch:\n",
    "            upload_batch_to_s3(S3_PREFIX, batch, batch_count)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during the process: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6da3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entry_id': 'http://arxiv.org/abs/2510.11713v1', 'title': 'Are Large Reasoning Models Interruptible?', 'authors': ['Tsung-Han Wu', 'Mihran Miroyan', 'David M. Chan', 'Trevor Darrell', 'Narges Norouzi', 'Joseph E. Gonzalez'], 'summary': 'Large Reasoning Models (LRMs) excel at complex reasoning but are\\ntraditionally evaluated in static, \"frozen world\" settings: model responses are\\nassumed to be instantaneous, and the context of a request is presumed to be\\nimmutable over the duration of the response. While generally true for\\nshort-term tasks, the \"frozen world\" assumption breaks down in modern reasoning\\ntasks such as assistive programming, where models may take hours to think\\nthrough problems and code may change dramatically from the time the model\\nstarts thinking to the model\\'s final output. In this work, we challenge the\\nfrozen world assumption and evaluate LRM robustness under two realistic dynamic\\nscenarios: interruptions, which test the quality of the model\\'s partial outputs\\non a limited budget, and dynamic context, which tests model adaptation to\\nin-flight changes. Across mathematics and programming benchmarks that require\\nlong-form reasoning, static evaluations consistently overestimate robustness:\\neven state-of-the-art LRMs, which achieve high accuracy in static settings, can\\nfail unpredictably when interrupted or exposed to changing context, with\\nperformance dropping by up to 60% when updates are introduced late in the\\nreasoning process. Our analysis further reveals several novel failure modes,\\nincluding reasoning leakage, where models fold the reasoning into their final\\nanswer when interrupted; panic, where under time pressure models abandon\\nreasoning entirely and return incorrect answers; and self-doubt, where\\nperformance degrades while incorporating updated information.', 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.LG'], 'published': '2025-10-13T17:59:35+00:00', 'updated': '2025-10-13T17:59:35+00:00', 'journal_ref': None, 'doi': None, 'published_date': '2025-10-13', 'updated_date': '2025-10-13', 'etl_datetime': '2025-10-14 12:00:27'}\n",
      "{'entry_id': 'http://arxiv.org/abs/2510.11711v1', 'title': 'Reinforced sequential Monte Carlo for amortised sampling', 'authors': ['Sanghyeok Choi', 'Sarthak Mittal', 'Víctor Elvira', 'Jinkyoo Park', 'Nikolay Malkin'], 'summary': 'This paper proposes a synergy of amortised and particle-based methods for\\nsampling from distributions defined by unnormalised density functions. We state\\na connection between sequential Monte Carlo (SMC) and neural sequential\\nsamplers trained by maximum-entropy reinforcement learning (MaxEnt RL), wherein\\nlearnt sampling policies and value functions define proposal kernels and twist\\nfunctions. Exploiting this connection, we introduce an off-policy RL training\\nprocedure for the sampler that uses samples from SMC -- using the learnt\\nsampler as a proposal -- as a behaviour policy that better explores the target\\ndistribution. We describe techniques for stable joint training of proposals and\\ntwist functions and an adaptive weight tempering scheme to reduce training\\nsignal variance. Furthermore, building upon past attempts to use experience\\nreplay to guide the training of neural samplers, we derive a way to combine\\nhistorical samples with annealed importance sampling weights within a replay\\nbuffer. On synthetic multi-modal targets (in both continuous and discrete\\nspaces) and the Boltzmann distribution of alanine dipeptide conformations, we\\ndemonstrate improvements in approximating the true distribution as well as\\ntraining stability compared to both amortised and Monte Carlo methods.', 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'stat.ML'], 'published': '2025-10-13T17:59:11+00:00', 'updated': '2025-10-13T17:59:11+00:00', 'journal_ref': None, 'doi': None, 'published_date': '2025-10-13', 'updated_date': '2025-10-13', 'etl_datetime': '2025-10-14 12:00:27'}\n",
      "{'entry_id': 'http://arxiv.org/abs/2510.11709v1', 'title': 'Adversarial Attacks Leverage Interference Between Features in Superposition', 'authors': ['Edward Stevinson', 'Lucas Prieto', 'Melih Barsbey', 'Tolga Birdal'], 'summary': \"Fundamental questions remain about when and why adversarial examples arise in\\nneural networks, with competing views characterising them either as artifacts\\nof the irregularities in the decision landscape or as products of sensitivity\\nto non-robust input features. In this paper, we instead argue that adversarial\\nvulnerability can stem from efficient information encoding in neural networks.\\nSpecifically, we show how superposition - where networks represent more\\nfeatures than they have dimensions - creates arrangements of latent\\nrepresentations that adversaries can exploit. We demonstrate that adversarial\\nperturbations leverage interference between superposed features, making attack\\npatterns predictable from feature arrangements. Our framework provides a\\nmechanistic explanation for two known phenomena: adversarial attack\\ntransferability between models with similar training regimes and class-specific\\nvulnerability patterns. In synthetic settings with precisely controlled\\nsuperposition, we establish that superposition suffices to create adversarial\\nvulnerability. We then demonstrate that these findings persist in a ViT trained\\non CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct\\nof networks' representational compression, rather than flaws in the learning\\nprocess or non-robust inputs.\", 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.AI', 'cs.CV'], 'published': '2025-10-13T17:59:02+00:00', 'updated': '2025-10-13T17:59:02+00:00', 'journal_ref': None, 'doi': None, 'published_date': '2025-10-13', 'updated_date': '2025-10-13', 'etl_datetime': '2025-10-14 12:00:27'}\n",
      "{'entry_id': 'http://arxiv.org/abs/2510.11696v1', 'title': 'QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs', 'authors': ['Wei Huang', 'Yi Ge', 'Shuai Yang', 'Yicheng Xiao', 'Huizi Mao', 'Yujun Lin', 'Hanrong Ye', 'Sifei Liu', 'Ka Chun Cheung', 'Hongxu Yin', 'Yao Lu', 'Xiaojuan Qi', 'Song Han', 'Yukang Chen'], 'summary': \"We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\\nlong rollout durations. QeRL addresses these issues by combining NVFP4\\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\\nwhile reducing memory overhead. Beyond efficiency, our findings show that\\nquantization noise increases policy entropy, enhancing exploration, and\\nenabling the discovery of better strategies during RL. To further optimize\\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\\nwhich dynamically adjusts noise during training. Experiments demonstrate that\\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\\nGPU, while delivering overall speedups for RL training. It also achieves faster\\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\\nmatching the performance of full-parameter fine-tuning on mathematical\\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\\nresults establish QeRL as an efficient and effective framework for RL training\\nin LLMs.\", 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.CL', 'cs.CV'], 'published': '2025-10-13T17:55:09+00:00', 'updated': '2025-10-13T17:55:09+00:00', 'journal_ref': None, 'doi': None, 'published_date': '2025-10-13', 'updated_date': '2025-10-13', 'etl_datetime': '2025-10-14 12:00:27'}\n",
      "{'entry_id': 'http://arxiv.org/abs/2510.11691v1', 'title': 'Tight Regret Upper and Lower Bounds for Optimistic Hedge in Two-Player Zero-Sum Games', 'authors': ['Taira Tsuchiya'], 'summary': \"In two-player zero-sum games, the learning dynamic based on optimistic Hedge\\nachieves one of the best-known regret upper bounds among strongly-uncoupled\\nlearning dynamics. With an appropriately chosen learning rate, the social and\\nindividual regrets can be bounded by $O(\\\\log(mn))$ in terms of the numbers of\\nactions $m$ and $n$ of the two players. This study investigates the optimality\\nof the dependence on $m$ and $n$ in the regret of optimistic Hedge. To this\\nend, we begin by refining existing regret analysis and show that, in the\\nstrongly-uncoupled setting where the opponent's number of actions is known,\\nboth the social and individual regret bounds can be improved to $O(\\\\sqrt{\\\\log m\\n\\\\log n})$. In this analysis, we express the regret upper bound as an\\noptimization problem with respect to the learning rates and the coefficients of\\ncertain negative terms, enabling refined analysis of the leading constants. We\\nthen show that the existing social regret bound as well as these new social and\\nindividual regret upper bounds cannot be further improved for optimistic Hedge\\nby providing algorithm-dependent individual regret lower bounds. Importantly,\\nthese social regret upper and lower bounds match exactly including the constant\\nfactor in the leading term. Finally, building on these results, we improve the\\nlast-iterate convergence rate and the dynamic regret of a learning dynamic\\nbased on optimistic Hedge, and complement these bounds with algorithm-dependent\\ndynamic regret lower bounds that match the improved bounds.\", 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.GT', 'stat.ML'], 'published': '2025-10-13T17:52:01+00:00', 'updated': '2025-10-13T17:52:01+00:00', 'journal_ref': None, 'doi': None, 'published_date': '2025-10-13', 'updated_date': '2025-10-13', 'etl_datetime': '2025-10-14 12:00:27'}\n",
      "{'entry_id': 'http://arxiv.org/abs/2510.11690v1', 'title': 'Diffusion Transformers with Representation Autoencoders', 'authors': ['Boyang Zheng', 'Nanye Ma', 'Shengbang Tong', 'Saining Xie'], 'summary': 'Latent generative modeling, where a pretrained autoencoder maps pixels into a\\nlatent space for the diffusion process, has become the standard strategy for\\nDiffusion Transformers (DiT); however, the autoencoder component has barely\\nevolved. Most DiTs continue to rely on the original VAE encoder, which\\nintroduces several limitations: outdated backbones that compromise\\narchitectural simplicity, low-dimensional latent spaces that restrict\\ninformation capacity, and weak representations that result from purely\\nreconstruction-based training and ultimately limit generative quality. In this\\nwork, we explore replacing the VAE with pretrained representation encoders\\n(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term\\nRepresentation Autoencoders (RAEs). These models provide both high-quality\\nreconstructions and semantically rich latent spaces, while allowing for a\\nscalable transformer-based architecture. Since these latent spaces are\\ntypically high-dimensional, a key challenge is enabling diffusion transformers\\nto operate effectively within them. We analyze the sources of this difficulty,\\npropose theoretically motivated solutions, and validate them empirically. Our\\napproach achieves faster convergence without auxiliary representation alignment\\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\\nachieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no\\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\\nclear advantages and should be the new default for diffusion transformer\\ntraining.', 'primary_category': 'cs.CV', 'categories': ['cs.CV', 'cs.LG'], 'published': '2025-10-13T17:51:39+00:00', 'updated': '2025-10-13T17:51:39+00:00', 'journal_ref': None, 'doi': None, 'published_date': '2025-10-13', 'updated_date': '2025-10-13', 'etl_datetime': '2025-10-14 12:00:27'}\n",
      "{'entry_id': 'http://arxiv.org/abs/2510.11686v1', 'title': 'Representation-Based Exploration for Language Models: From Test-Time to Post-Training', 'authors': ['Jens Tuyls', 'Dylan J. Foster', 'Akshay Krishnamurthy', 'Jordan T. Ash'], 'summary': \"Reinforcement learning (RL) promises to expand the capabilities of language\\nmodels, but it is unclear if current RL techniques promote the discovery of\\nnovel behaviors, or simply sharpen those already present in the base model. In\\nthis paper, we investigate the value of deliberate exploration -- explicitly\\nincentivizing the model to discover novel and diverse behaviors -- and aim to\\nunderstand how the knowledge in pre-trained models can guide this search. Our\\nmain finding is that exploration with a simple, principled,\\nrepresentation-based bonus derived from the pre-trained language model's hidden\\nstates significantly improves diversity and pass@k rates -- both for\\npost-training, and in a novel inference-time scaling setting we introduce. For\\ninference-time, exploration with representation-based diversity improves\\nefficiency, consistently improving pass@k rates across a variety of models and\\nreasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50%\\nimprovement in verifier efficiency on almost all tasks. For post-training, we\\nshow that integrating this exploration strategy into an RL pipeline improves\\nreasoning performance over that of the initial model and over standard RL\\npost-training. For example, on AIME 2024, our post-trained\\nQwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model,\\ndemonstrating a 3x improvement in test-time sample efficiency. Overall, our\\nfindings suggest that deliberate exploration -- with the right notion of\\ndiversity -- is a practical path toward discovery of new behaviors beyond\\nsharpening.\", 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.AI'], 'published': '2025-10-13T17:49:05+00:00', 'updated': '2025-10-13T17:49:05+00:00', 'journal_ref': None, 'doi': None, 'published_date': '2025-10-13', 'updated_date': '2025-10-13', 'etl_datetime': '2025-10-14 12:00:27'}\n",
      "{'entry_id': 'http://arxiv.org/abs/2510.11683v1', 'title': 'Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models', 'authors': ['Nianyi Lin', 'Jiajie Zhang', 'Lei Hou', 'Juanzi Li'], 'summary': 'A key challenge in applying reinforcement learning (RL) to diffusion large\\nlanguage models (dLLMs) lies in the intractability of their likelihood\\nfunctions, which are essential for the RL objective, necessitating\\ncorresponding approximation in each training step. While existing methods\\napproximate the log-likelihoods by their evidence lower bounds (ELBOs) via\\ncustomized Monte Carlo (MC) sampling, the forward computational graphs of all\\nMC samples need to be retained for the gradient computation of non-linear terms\\nin the RL objective, resulting in significant memory overhead. This constraint\\nrestricts feasible sample sizes, leading to imprecise likelihood approximations\\nand ultimately distorting the RL objective. To overcome this limitation, we\\npropose \\\\emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient\\nRL algorithm that maximizes a specially constructed lower bound of the\\nELBO-based objective. This lower bound is carefully designed to satisfy two key\\nproperties: (1) Linearity: it is formulated in a linear sum where each term\\ndepends only on a single MC sample, thereby enabling gradient accumulation\\nacross samples and ensuring constant memory usage; (2) Equivalence: Both the\\nvalue and gradient of this lower bound are equal to those of the ELBO-based\\nobjective in on-policy training, making it also an effective approximation for\\nthe original RL objective. These properties allow BGPO to adopt a large MC\\nsample size, resulting in more accurate likelihood approximations and improved\\nRL objective estimation, which in turn leads to enhanced performance.\\nExperiments show that BGPO significantly outperforms previous RL algorithms for\\ndLLMs in math problem solving, code generation, and planning tasks.', 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'cs.AI', 'cs.CL'], 'published': '2025-10-13T17:47:50+00:00', 'updated': '2025-10-13T17:47:50+00:00', 'journal_ref': None, 'doi': None, 'published_date': '2025-10-13', 'updated_date': '2025-10-13', 'etl_datetime': '2025-10-14 12:00:27'}\n",
      "{'entry_id': 'http://arxiv.org/abs/2510.11677v1', 'title': 'Chronologically Consistent Generative AI', 'authors': ['Songrun He', 'Linying Lv', 'Asaf Manela', 'Jimmy Wu'], 'summary': 'We introduce a family of chronologically consistent, instruction-following\\nlarge language models to eliminate lookahead bias. Each model is trained only\\non data available before a clearly defined knowledge-cutoff date, ensuring\\nstrict temporal separation from any post-cutoff data. The resulting framework\\noffers (i) a simple, conversational chat interface, (ii) fully open, fixed\\nmodel weights that guarantee replicability, and (iii) a conservative lower\\nbound on forecast accuracy, isolating the share of predictability that survives\\nonce training leakage is removed. Together, these features provide researchers\\nwith an easy-to-use generative AI tool useful for a wide range of prediction\\ntasks that is free of lookahead bias.', 'primary_category': 'cs.LG', 'categories': ['cs.LG', 'q-fin.GN'], 'published': '2025-10-13T17:45:24+00:00', 'updated': '2025-10-13T17:45:24+00:00', 'journal_ref': None, 'doi': None, 'published_date': '2025-10-13', 'updated_date': '2025-10-13', 'etl_datetime': '2025-10-14 12:00:27'}\n",
      "{'entry_id': 'http://arxiv.org/abs/2510.11676v1', 'title': 'Accelerated stochastic first-order method for convex optimization under heavy-tailed noise', 'authors': ['Chuan He', 'Zhaosong Lu'], 'summary': 'We study convex composite optimization problems, where the objective function\\nis given by the sum of a prox-friendly function and a convex function whose\\nsubgradients are estimated under heavy-tailed noise. Existing work often\\nemploys gradient clipping or normalization techniques in stochastic first-order\\nmethods to address heavy-tailed noise. In this paper, we demonstrate that a\\nvanilla stochastic algorithm -- without additional modifications such as\\nclipping or normalization -- can achieve optimal complexity for these problems.\\nIn particular, we establish that an accelerated stochastic proximal subgradient\\nmethod achieves a first-order oracle complexity that is universally optimal for\\nsmooth, weakly smooth, and nonsmooth convex optimization, as well as for\\nstochastic convex optimization under heavy-tailed noise. Numerical experiments\\nare further provided to validate our theoretical results.', 'primary_category': 'math.OC', 'categories': ['math.OC', 'cs.AI', 'cs.LG', 'stat.ML', '49M05, 49M37, 90C25, 90C30'], 'published': '2025-10-13T17:45:05+00:00', 'updated': '2025-10-13T17:45:05+00:00', 'journal_ref': None, 'doi': None, 'published_date': '2025-10-13', 'updated_date': '2025-10-13', 'etl_datetime': '2025-10-14 12:00:27'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "input_file = Path(\"arxiv_data/arxiv_batch_1.json\")\n",
    "output_file = Path(\"arxiv_data/arxiv_batch_cleaned.json\")\n",
    "\n",
    "def transform_datetime2date(dt_str):\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(dt_str.replace(\"Z\", \"+00:00\"))\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "# 去重\n",
    "unique_papers = {paper[\"entry_id\"]: paper for paper in papers}\n",
    "\n",
    "\n",
    "required_fields = [\n",
    "    \"entry_id\", \"title\", \"summary\", \"authors\", \n",
    "    \"primary_category\", \"published\", \"updated\"\n",
    "]\n",
    "\n",
    "cleaned_papers = []\n",
    "for paper in unique_papers.values():\n",
    "    # 刪除缺值資料\n",
    "    if all(paper.get(field) for field in required_fields) and all(a.strip() for a in paper[\"authors\"]):\n",
    "        paper[\"published_date\"] = transform_datetime2date(paper[\"published\"])\n",
    "        paper[\"updated_date\"] = transform_datetime2date(paper[\"updated\"])\n",
    "        paper[\"etl_datetime\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\") # use UTC timezone\n",
    "        cleaned_papers.append(paper)\n",
    "\n",
    "for paper in cleaned_papers:\n",
    "    print(paper)\n",
    "\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(cleaned_papers, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(f\"清理完成，共 {len(cleaned_papers)} 筆，已儲存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ce2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# 建立 DynamoDB 連線\n",
    "dynamodb = boto3.resource(\n",
    "    'dynamodb',\n",
    "    aws_access_key_id='',\n",
    "    aws_secret_access_key='',\n",
    "    region_name='ap-southeast-2'\n",
    ")\n",
    "\n",
    "# 指定 table 名稱\n",
    "table = dynamodb.Table('download_paper_entry_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5adf2a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已新增\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "entry_id = \"http://arxiv.org/abs/2510.11683v1\"\n",
    "item = {\n",
    "    \"entry_id\": entry_id,\n",
    "    \"status\": \"uploaded\",  # \"failed\"\n",
    "    \"last_attempt\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"error_msg\": \"\" \n",
    "}\n",
    "\n",
    "try:\n",
    "    table.put_item(\n",
    "        Item=item,\n",
    "        ConditionExpression='attribute_not_exists(entry_id)'\n",
    "    )\n",
    "    print(\"已新增\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ConditionalCheckFailedException':\n",
    "        print(\"這篇 paper 已存在\")\n",
    "    else:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60adb45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已存在 {'entry_id': 'http://arxiv.org/abs/2510.11683v1', 'last_attempt': '2025-10-14 15:15:24', 'error_msg': '', 'status': 'uploaded'}\n"
     ]
    }
   ],
   "source": [
    "response = table.get_item(Key={'entry_id': entry_id})\n",
    "item = response.get('Item')\n",
    "\n",
    "if item:\n",
    "    print(\"已存在\", item)\n",
    "else:\n",
    "    print(\"不存在\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c81b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/2510.11683v1\n"
     ]
    }
   ],
   "source": [
    "print(entry_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "deb0eed3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     s3.put_object(Bucket=bucket_name, Key=(prefix + \u001b[33m\"\u001b[39m\u001b[33m.keep\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[43mcreate_s3_bucket_and_prefix\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmy-test-bucket\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcs.LG\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mcreate_s3_bucket_and_prefix\u001b[39m\u001b[34m(bucket_name, domain)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_s3_bucket_and_prefix\u001b[39m(bucket_name: \u001b[38;5;28mstr\u001b[39m, domain: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     env_path = os.path.join(os.path.dirname(\u001b[34;43m__file__\u001b[39;49m), \u001b[33m\"\u001b[39m\u001b[33m../.env\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(env_path):\n\u001b[32m      8\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.env not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "def create_s3_bucket_and_prefix(bucket_name: str, domain: str):\n",
    "    env_path = os.path.join(os.path.dirname(__file__), \"../.env\")\n",
    "    if not os.path.exists(env_path):\n",
    "        raise FileNotFoundError(f\".env not found at {env_path}\")\n",
    "    \n",
    "    load_dotenv(env_path)\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        region_name=os.getenv(\"AWS_REGION\"),\n",
    "        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    )\n",
    "\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "    prefix = f\"raw/domain={domain}/\"\n",
    "    s3.put_object(Bucket=bucket_name, Key=(prefix + \".keep\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_s3_bucket_and_prefix(\"my-test-bucket\", \"cs.LG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2ab9f8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hackmd-paper-bucket\n"
     ]
    }
   ],
   "source": [
    "# 查看你有哪個 Bucket\n",
    "\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "response = s3.list_buckets()\n",
    "for bucket in response[\"Buckets\"]:\n",
    "    print(bucket[\"Name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "407ee619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefixes:\n",
      "raw/domain=cs.LG/\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=\"raw/\",   # 只看 raw/ 底下\n",
    "    Delimiter=\"/\"\n",
    ")\n",
    "\n",
    "if \"CommonPrefixes\" in response:\n",
    "    print(\"Prefixes:\")\n",
    "    for prefix in response[\"CommonPrefixes\"]:\n",
    "        print(prefix[\"Prefix\"])\n",
    "else:\n",
    "    print(\"沒有找到任何 prefix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "68657f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已上傳 /home/hank/hackmd-data-pipeline/tests/arxiv_data/arxiv_batch_2.json 到 S3: raw/domain=cs.LG/arxiv_batch_2.json\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "prefix = \"raw/domain=cs.LG/\"\n",
    "local_file = \"/home/hank/hackmd-data-pipeline/tests/arxiv_data/arxiv_batch_2.json\"\n",
    "key = prefix + os.path.basename(local_file)\n",
    "\n",
    "with open(local_file, \"rb\") as f:\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=f,\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "print(f\"已上傳 {local_file} 到 S3: {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "302e5623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "檔案列表：\n",
      "raw/domain=cs.LG/arxiv_batch_2.json\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "prefix = \"raw/domain=cs.LG/\"\n",
    "\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=prefix,\n",
    "    Delimiter=\"/\" \n",
    ")\n",
    "\n",
    "if \"Contents\" in response:\n",
    "    print(\"檔案列表：\")\n",
    "    files = [obj[\"Key\"] for obj in response[\"Contents\"] if not obj[\"Key\"].endswith(\".keep\")]\n",
    "    for f in files:\n",
    "        print(f)\n",
    "else:\n",
    "    print(\"此 prefix 下沒有檔案\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
