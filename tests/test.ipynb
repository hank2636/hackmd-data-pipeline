{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c23e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 讀取環境變數\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c88f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 arxiv api 取資料\n",
    "import arxiv\n",
    "\n",
    "client = arxiv.Client(\n",
    "  page_size=1,\n",
    ")\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=\"\",\n",
    "    max_results=1,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    sort_order=arxiv.SortOrder.Descending,\n",
    "    id_list=[\"2301.12345v1\"]\n",
    ")\n",
    "\n",
    "result = next(client.results(search))\n",
    "for attr, value in vars(result).items():\n",
    "  print(attr,\":\",value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7638366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認 db 中有哪些資料需要處理\n",
    "\n",
    "from src.core.db import get_pg\n",
    "from src.core.pg_engine import PsqlEngine\n",
    "pg = get_pg()\n",
    "\n",
    "def get_pending_gz(pg: PsqlEngine, num: str) -> list:\n",
    "    stmt = f\"\"\"\n",
    "        select category,s3_path from etl.raw_batches where etl_status = 'pending' order by batch_id limit {num};\n",
    "    \"\"\"\n",
    "    result = pg.execute_query(stmt)\n",
    "    return result\n",
    "\n",
    "\n",
    "pending_gz = get_pending_gz(pg, 10)\n",
    "\n",
    "pending_gz = [r.__dict__ if hasattr(r, \"__dict__\") else dict(r._asdict()) for r in pending_gz]\n",
    "print(pending_gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c1e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from psycopg2.extras import Json\n",
    "import gzip\n",
    "import io\n",
    "import boto3\n",
    "import uuid\n",
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime, timezone\n",
    "from src.core.db import get_pg\n",
    "from src.core.pg_engine import PsqlEngine\n",
    "\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "def load_config(BUCKET_NAME):\n",
    "    bucket_name = BUCKET_NAME\n",
    "    key = \"config/config.yaml\"\n",
    "    local_path = \"/tmp/config.yaml\"\n",
    "\n",
    "    try:\n",
    "        s3.download_file(bucket_name, key, local_path)\n",
    "        with open(local_path, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load config from S3: {e}\")\n",
    "        raise\n",
    "\n",
    "cfg = load_config(BUCKET_NAME)\n",
    "\n",
    "# ETL 每次吃多少個 gz 檔\n",
    "PENDING_GZ_BATCH = cfg['etl']['pending_gz_batch']\n",
    "\n",
    "# ETL 多少筆資料一次寫入 DB\n",
    "ETL_BATCH_SIZE = cfg['etl']['etl_batch_size']\n",
    "\n",
    "\n",
    "pg = get_pg()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_pending_gz(pg: PsqlEngine, num: str) -> list:\n",
    "    stmt = f\"\"\"\n",
    "        select category, s3_path from etl.raw_batches where etl_status = 'pending' order by batch_id limit {num};\n",
    "    \"\"\"\n",
    "    result = pg.execute_query(stmt)\n",
    "    return result\n",
    "\n",
    "def parse_record(record: dict, s3_key) -> tuple:\n",
    "    published_date = record.get(\"published\")\n",
    "    updated_date = record.get(\"updated\")\n",
    "    if published_date:\n",
    "        published_date = datetime.fromisoformat(published_date).date()\n",
    "    if updated_date:\n",
    "        updated_date = datetime.fromisoformat(updated_date).date()\n",
    "    return (\n",
    "        record.get(\"entry_id\"),\n",
    "        record.get(\"title\"),\n",
    "        record.get(\"authors\", []),\n",
    "        json.dumps({}),\n",
    "        record.get(\"summary\"),\n",
    "        record.get(\"primary_category\"),\n",
    "        record.get(\"categories\", []),\n",
    "        record.get(\"published\"),\n",
    "        record.get(\"updated\"),\n",
    "        record.get(\"journal_ref\"),\n",
    "        record.get(\"doi\"),\n",
    "        json.dumps({}),\n",
    "        published_date,\n",
    "        updated_date,\n",
    "        datetime.now(timezone.utc),\n",
    "        1,\n",
    "        [],\n",
    "        None,\n",
    "        s3_key\n",
    "    )\n",
    "\n",
    "def parse_history_record(record: dict, s3_key: str, operation: str, etl_stage: str) -> tuple:\n",
    "    summary = record.get(\"summary\") or \"\"\n",
    "    summary = summary.replace('\\x00', '').replace('\\n', ' ').replace('\\r', ' ')\n",
    "    return (\n",
    "        str(uuid.uuid4()),\n",
    "        record.get(\"entry_id\"),\n",
    "        datetime.now(timezone.utc).timestamp(),\n",
    "        datetime.now(timezone.utc),\n",
    "        etl_stage,\n",
    "        record.get(\"title\"),\n",
    "        record.get(\"authors\", []),\n",
    "        Json({}),\n",
    "        summary,\n",
    "        record.get(\"primary_category\"),\n",
    "        record.get(\"categories\", []),\n",
    "        record.get(\"published\"),\n",
    "        record.get(\"updated\"),\n",
    "        record.get(\"journal_ref\"),\n",
    "        record.get(\"doi\"),\n",
    "        Json({}),\n",
    "        [],\n",
    "        None,\n",
    "        s3_key,\n",
    "        operation\n",
    "    )\n",
    "\n",
    "def safe_insert(table, batch):\n",
    "    try:\n",
    "        pg.insert_mogrify(table, batch)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Batch insert failed: {e}\")\n",
    "        for row in batch:\n",
    "            try:\n",
    "                pg.insert_mogrify(table, [row])\n",
    "            except Exception as e2:\n",
    "                logger.error(f\"Single insert failed for row {row[0]}: {e2}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def update_etl_status(pg: PsqlEngine, s3_path: str, status: str, started_at=None, finished_at=None, error_msg=None):\n",
    "    stmt = \"\"\"\n",
    "        update etl.raw_batches\n",
    "        set etl_status = %s,\n",
    "            etl_started_at = coalesce(%s, etl_started_at),\n",
    "            etl_finished_at = coalesce(%s, etl_finished_at),\n",
    "            error_msg = %s\n",
    "        where s3_path = %s;\n",
    "    \"\"\"\n",
    "    params = (status, started_at, finished_at, error_msg, s3_path)\n",
    "    pg.execute_cmd(stmt, params)\n",
    "\n",
    "def load_s3_gzip_to_pg(bucket: str, s3_key: str, etl_stage: str = \"initial_load\"):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=s3_key)\n",
    "    batch, batch_history = [], []\n",
    "\n",
    "    with gzip.GzipFile(fileobj=io.BytesIO(obj[\"Body\"].read()), mode=\"rb\") as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line.decode(\"utf-8\"))\n",
    "            batch.append(parse_record(record, s3_key))\n",
    "            batch_history.append(parse_history_record(record, s3_key, operation=\"insert\", etl_stage=etl_stage))\n",
    "            if len(batch) >= ETL_BATCH_SIZE:\n",
    "                safe_insert(\"arxiv_papers\", batch)\n",
    "                safe_insert(\"arxiv_papers_history\", batch_history)\n",
    "                batch, batch_history = [], []\n",
    "\n",
    "    if batch:\n",
    "        safe_insert(\"arxiv_papers\", batch)\n",
    "        safe_insert(\"arxiv_papers_history\", batch_history)\n",
    "    return datetime.now(timezone.utc)\n",
    "\n",
    "pending_gz = get_pending_gz(pg, PENDING_GZ_BATCH) # 取多少檔案下來\n",
    "pending_gz = [r.__dict__ if hasattr(r, \"__dict__\") else dict(r._asdict()) for r in pending_gz]\n",
    "\n",
    "for pending_gz_dict in pending_gz:\n",
    "    key = pending_gz_dict['s3_path']\n",
    "    logger.info(f\"Processing {key}\")\n",
    "    started_at = datetime.now(timezone.utc)\n",
    "    update_etl_status(pg, key, \"processing\", started_at=started_at)\n",
    "    try:\n",
    "        finished_at = load_s3_gzip_to_pg(BUCKET_NAME, key)\n",
    "        update_etl_status(pg, key, \"finished\", finished_at=finished_at)\n",
    "        # logger.info(f\"Finished {key}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {key}: {e}\", exc_info=True)\n",
    "        update_etl_status(pg, key, \"failed\", finished_at=datetime.now(timezone.utc), error_msg=str(e))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "input_file = Path(\"arxiv_data/arxiv_batch_1.json\")\n",
    "output_file = Path(\"arxiv_data/arxiv_batch_cleaned.json\")\n",
    "\n",
    "def transform_datetime2date(dt_str):\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(dt_str.replace(\"Z\", \"+00:00\"))\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "# 去重\n",
    "unique_papers = {paper[\"entry_id\"]: paper for paper in papers}\n",
    "\n",
    "\n",
    "required_fields = [\n",
    "    \"entry_id\", \"title\", \"summary\", \"authors\", \n",
    "    \"primary_category\", \"published\", \"updated\"\n",
    "]\n",
    "\n",
    "cleaned_papers = []\n",
    "for paper in unique_papers.values():\n",
    "    # 刪除缺值資料\n",
    "    if all(paper.get(field) for field in required_fields) and all(a.strip() for a in paper[\"authors\"]):\n",
    "        paper[\"published_date\"] = transform_datetime2date(paper[\"published\"])\n",
    "        paper[\"updated_date\"] = transform_datetime2date(paper[\"updated\"])\n",
    "        paper[\"etl_datetime\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\") # use UTC timezone\n",
    "        cleaned_papers.append(paper)\n",
    "\n",
    "for paper in cleaned_papers:\n",
    "    print(paper)\n",
    "\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(cleaned_papers, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(f\"清理完成，共 {len(cleaned_papers)} 筆，已儲存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ce2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# 建立 DynamoDB 連線\n",
    "dynamodb = boto3.resource(\n",
    "    \"dynamodb\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "table = dynamodb.Table('download_paper_entry_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 新增一筆資料\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "entry_id = \"http://arxiv.org/abs/2510.11683v1\"\n",
    "item = {\n",
    "    \"category\": \"cs.LG\",\n",
    "    \"entry_id\": entry_id,\n",
    "    \"status\": \"uploaded\",  # \"failed\"\n",
    "    \"last_attempt\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"error_msg\": \"\" \n",
    "}\n",
    "\n",
    "try:\n",
    "    table.put_item(\n",
    "        Item=item,\n",
    "        ConditionExpression='attribute_not_exists(entry_id)'\n",
    "    )\n",
    "    print(\"已新增\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ConditionalCheckFailedException':\n",
    "        print(\"這篇 paper 已存在\")\n",
    "    else:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "def create_s3_bucket_and_prefix(bucket_name: str, domain: str):\n",
    "    env_path = os.path.join(os.path.dirname(__file__), \"../.env\")\n",
    "    if not os.path.exists(env_path):\n",
    "        raise FileNotFoundError(f\".env not found at {env_path}\")\n",
    "    \n",
    "    load_dotenv(env_path)\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        region_name=os.getenv(\"AWS_REGION\"),\n",
    "        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    )\n",
    "\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "    prefix = f\"raw/domain={domain}/\"\n",
    "    s3.put_object(Bucket=bucket_name, Key=(prefix + \".keep\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_s3_bucket_and_prefix(\"my-test-bucket\", \"cs.LG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看你有哪個 Bucket\n",
    "\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "response = s3.list_buckets()\n",
    "for bucket in response[\"Buckets\"]:\n",
    "    print(bucket[\"Name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ee619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"arvix-paper-bucket\"\n",
    "\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=\"raw/\",   # 只看 raw/ 底下\n",
    "    Delimiter=\"/\"\n",
    ")\n",
    "\n",
    "if \"CommonPrefixes\" in response:\n",
    "    print(\"Prefixes:\")\n",
    "    for prefix in response[\"CommonPrefixes\"]:\n",
    "        print(prefix[\"Prefix\"])\n",
    "else:\n",
    "    print(\"沒有找到任何 prefix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101f67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上傳 config.yaml 至 S3\n",
    "\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"arvix-paper-bucket\"\n",
    "local_file = \"../config/config.yaml\"\n",
    "key = \"config/config.yaml\"\n",
    "\n",
    "with open(local_file, \"rb\") as f:\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=f,\n",
    "        ContentType=\"application/x-yaml\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 S3 上的 config.yaml\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"arvix-paper-bucket\"\n",
    "local_path = \"/tmp/config.yaml\"\n",
    "key = \"config/config.yaml\"\n",
    "s3.download_file(bucket_name, key, local_path)\n",
    "\n",
    "with open(local_path) as f:\n",
    "    data = f.read()\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68657f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "prefix = \"raw/domain=cs.LG/\"\n",
    "local_file = \"/home/hank/hackmd-data-pipeline/tests/arxiv_data/arxiv_batch_2.json\"\n",
    "key = prefix + os.path.basename(local_file)\n",
    "\n",
    "with open(local_file, \"rb\") as f:\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=f,\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "print(f\"已上傳 {local_file} 到 S3: {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "prefix = \"raw/domain=cs.LG/\"\n",
    "\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=prefix,\n",
    "    Delimiter=\"/\" \n",
    ")\n",
    "\n",
    "if \"Contents\" in response:\n",
    "    print(\"檔案列表：\")\n",
    "    files = [obj[\"Key\"] for obj in response[\"Contents\"] if not obj[\"Key\"].endswith(\".keep\")]\n",
    "    for f in files:\n",
    "        print(f)\n",
    "else:\n",
    "    print(\"此 prefix 下沒有檔案\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.db import get_pg\n",
    "from src.core.pg_engine import PsqlEngine\n",
    "pg = get_pg()\n",
    "\n",
    "def paper_exists(pg: PsqlEngine, category: str, entry_id: str) -> bool:\n",
    "    stmt = f\"\"\"\n",
    "        SELECT 1\n",
    "        FROM papers.downloaded_papers\n",
    "        WHERE category = '{category}' AND entry_id = '{entry_id}'\n",
    "        LIMIT 1;\n",
    "    \"\"\"\n",
    "    result = pg.execute_query(stmt)\n",
    "    return bool(result)\n",
    "\n",
    "\n",
    "paper_exists(pg, \"cs_LG\",'dsfd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb314a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手動觸發 Collector lambda, 會取得完整的執行紀錄\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import boto3\n",
    "import requests\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "region_name = os.getenv(\"AWS_REGION\")\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=region_name\n",
    ")\n",
    "credentials = session.get_credentials().get_frozen_credentials()\n",
    "\n",
    "region = \"ap-northeast-1\"\n",
    "service = \"execute-api\"\n",
    "\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key,\n",
    "    credentials.secret_key,\n",
    "    region,\n",
    "    service,\n",
    "    session_token=credentials.token\n",
    ")\n",
    "\n",
    "url = \"https://z1cft4uc6g.execute-api.ap-northeast-1.amazonaws.com/default/Collector\"\n",
    "payload = {\"trigger\": \"manual\"}\n",
    "\n",
    "response = requests.post(url, json=payload, auth=awsauth)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Body:\", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8b2b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'c0ee223a-6843-4465-813a-ba39a21d4d5a', 'HTTPStatusCode': 202, 'HTTPHeaders': {'date': 'Wed, 22 Oct 2025 07:52:21 GMT', 'content-length': '0', 'connection': 'keep-alive', 'x-amzn-requestid': 'c0ee223a-6843-4465-813a-ba39a21d4d5a', 'x-amzn-remapped-content-length': '0', 'x-amzn-trace-id': 'root=1-68f88d34-177d20e372effc102bef68e2;parent=28ee1fc2b4827594;sampled=0'}, 'RetryAttempts': 0}, 'StatusCode': 202, 'Payload': <botocore.response.StreamingBody object at 0x7f500e5eb730>}\n"
     ]
    }
   ],
   "source": [
    "# 手動觸發 Collector lambda \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "region_name = \"ap-northeast-1\"\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "lambda_client = boto3.client(\n",
    "    \"lambda\",\n",
    "    region_name=region_name,\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "response = lambda_client.invoke(\n",
    "    FunctionName=\"Collector\", # Collector ETL_A ETL_B\n",
    "    InvocationType=\"Event\",\n",
    "    Payload=json.dumps({\"trigger\": \"manual\"}).encode()\n",
    ")\n",
    "\n",
    "\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackmd-data-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
