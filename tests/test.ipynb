{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37300cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client(\n",
    "  page_size=1,\n",
    ")\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=\"\",\n",
    "    max_results=1,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    sort_order=arxiv.SortOrder.Descending,\n",
    "    id_list=[\"2301.12345v1\"]\n",
    ")\n",
    "\n",
    "result = next(client.results(search))\n",
    "for attr, value in vars(result).items():\n",
    "  print(attr,\":\",value)\n",
    "    \n",
    "# print(\"ID:\", result.entry_id)\n",
    "# print(\"標題:\", result.title)\n",
    "# print(\"摘要:\", result.summary)\n",
    "# print(\"作者:\", \", \".join(str(a) for a in result.authors))\n",
    "# print(\"主分類:\", result.primary_category)\n",
    "# print(\"其他分類:\", \", \".join(result.categories))\n",
    "# print(\"發表日期:\", result.published)\n",
    "# print(\"更新日期:\", result.updated)\n",
    "# print(\"期刊/會議資訊:\", result.journal_ref)\n",
    "# print(\"DOI:\", result.doi)\n",
    "# print(\"PDF_url:\", result.pdf_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import boto3\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "logging.getLogger(\"arxiv\").setLevel(logging.WARNING)\n",
    "\n",
    "category_list = [\"cs.DS\", \"cs.AI\", \"cs.LG\", \"cs.CV\", \"cs.CL\", \"stat.ML\", \"math.ST\"]\n",
    "MAX_RESULTS_GOAL = 1000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "S3_BUCKET = \"hackmd-paper-bucket\"\n",
    "TABLE_NAME = \"download_paper_entry_id\"\n",
    "\n",
    "MAX_ATTEMPTS = 3\n",
    "INITIAL_DELAY_SECONDS = 5\n",
    "\n",
    "last_exception = None\n",
    "\n",
    "client = arxiv.Client(\n",
    "    page_size=MAX_RESULTS_GOAL, \n",
    "    delay_seconds=3,\n",
    "    num_retries=3\n",
    ")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    )\n",
    "\n",
    "dynamodb = boto3.resource(\n",
    "    \"dynamodb\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "table = dynamodb.Table(TABLE_NAME)\n",
    "\n",
    "def paper_exists(category, entry_id):\n",
    "    try:\n",
    "        response = table.get_item(\n",
    "            Key={\"category\": category, \"entry_id\": entry_id}\n",
    "        )\n",
    "        return \"Item\" in response\n",
    "    except Exception as e:\n",
    "        print(f\"DynamoDB query error: {e}\")\n",
    "        return False\n",
    "\n",
    "def record_to_dynamo(category, entry_id, status, error_msg=\"\"):\n",
    "    item = {\n",
    "        \"category\": category,\n",
    "        \"entry_id\": entry_id,\n",
    "        \"status\": status,  # \"uploaded\" 或 \"failed\"\n",
    "        \"last_attempt\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"error_msg\": error_msg\n",
    "    }\n",
    "    try:\n",
    "        table.put_item(\n",
    "            Item=item,\n",
    "            ConditionExpression=\"attribute_not_exists(entry_id)\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 寫入 DynamoDB 失敗 (entry_id={entry_id}): {e}\")\n",
    "        \n",
    "def upload_batch_to_s3(s3_prefix, batch_data, batch_num):\n",
    "    if not batch_data:\n",
    "        return\n",
    "    \n",
    "    jsonl_content = \"\\n\".join([json.dumps(paper, ensure_ascii=False) for paper in batch_data])\n",
    "    \n",
    "    utc_now = datetime.now(timezone.utc)\n",
    "    today_str = utc_now.strftime(\"%Y-%m-%d\")\n",
    "    utc_timestamp = int(utc_now.timestamp())\n",
    "    s3_key = f\"{s3_prefix}{today_str}/{category.replace('.','_')}_batch_{batch_num}_{utc_timestamp}.jsonl\"\n",
    "    \n",
    "    for attempt in range(MAX_ATTEMPTS):\n",
    "        try:\n",
    "            s3.put_object(\n",
    "                Bucket=S3_BUCKET,\n",
    "                Key=s3_key,\n",
    "                Body=jsonl_content.encode('utf-8'),\n",
    "                ContentType='application/jsonl'\n",
    "            )\n",
    "            print(f\"Successfully uploaded batch {batch_num} ({len(batch_data)} papers) to s3://{S3_BUCKET}/{s3_key}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_exception = e\n",
    "            print(f\"Attempt {attempt + 1}/{MAX_ATTEMPTS} failed for batch {batch_num}. Error: {e}\")\n",
    "            if attempt < MAX_ATTEMPTS - 1:\n",
    "                delay = INITIAL_DELAY_SECONDS * (2 ** attempt)\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"All {MAX_ATTEMPTS} attempts failed for batch {batch_num}. Could not upload.\")\n",
    "                raise last_exception\n",
    "\n",
    "try:\n",
    "    for category in category_list:\n",
    "        S3_PREFIX = f\"raw/{category.replace('.','_')}/\"\n",
    "        \n",
    "        search = arxiv.Search(\n",
    "            query=f'cat:{category}',\n",
    "            max_results=MAX_RESULTS_GOAL,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "            sort_order=arxiv.SortOrder.Descending\n",
    "        )\n",
    "        \n",
    "        results_generator = client.results(search)\n",
    "        \n",
    "        batch = []\n",
    "        batch_count = 0\n",
    "        \n",
    "        for paper_result in results_generator:\n",
    "            entry_id = paper_result.entry_id\n",
    "            if paper_exists(category, entry_id):\n",
    "                continue\n",
    "            paper_data = {\n",
    "                \"entry_id\": entry_id,\n",
    "                \"title\": paper_result.title,\n",
    "                \"authors\": [a.name for a in paper_result.authors],\n",
    "                \"summary\": paper_result.summary,\n",
    "                \"primary_category\": paper_result.primary_category,\n",
    "                \"categories\": paper_result.categories,\n",
    "                \"published\": paper_result.published.isoformat(),\n",
    "                \"updated\": paper_result.updated.isoformat(),\n",
    "                \"journal_ref\": paper_result.journal_ref,\n",
    "                \"doi\": paper_result.doi\n",
    "            }\n",
    "            batch.append(paper_data)\n",
    "            \n",
    "            record_to_dynamo(category, entry_id, status=\"pending\")\n",
    "            \n",
    "            if len(batch) >= BATCH_SIZE:\n",
    "                try:\n",
    "                    upload_batch_to_s3(S3_PREFIX, batch, batch_count)\n",
    "                    for paper in batch:\n",
    "                        record_to_dynamo(category, paper[\"entry_id\"], status=\"uploaded\")\n",
    "                except Exception as e:\n",
    "                    for paper in batch:\n",
    "                        record_to_dynamo(category, paper[\"entry_id\"], status=\"failed\", error_msg=str(e))\n",
    "                finally:\n",
    "                    batch = []\n",
    "                    batch_count += 1\n",
    "                \n",
    "        if batch:\n",
    "            upload_batch_to_s3(S3_PREFIX, batch, batch_count)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during the process: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "input_file = Path(\"arxiv_data/arxiv_batch_1.json\")\n",
    "output_file = Path(\"arxiv_data/arxiv_batch_cleaned.json\")\n",
    "\n",
    "def transform_datetime2date(dt_str):\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(dt_str.replace(\"Z\", \"+00:00\"))\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "# 去重\n",
    "unique_papers = {paper[\"entry_id\"]: paper for paper in papers}\n",
    "\n",
    "\n",
    "required_fields = [\n",
    "    \"entry_id\", \"title\", \"summary\", \"authors\", \n",
    "    \"primary_category\", \"published\", \"updated\"\n",
    "]\n",
    "\n",
    "cleaned_papers = []\n",
    "for paper in unique_papers.values():\n",
    "    # 刪除缺值資料\n",
    "    if all(paper.get(field) for field in required_fields) and all(a.strip() for a in paper[\"authors\"]):\n",
    "        paper[\"published_date\"] = transform_datetime2date(paper[\"published\"])\n",
    "        paper[\"updated_date\"] = transform_datetime2date(paper[\"updated\"])\n",
    "        paper[\"etl_datetime\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\") # use UTC timezone\n",
    "        cleaned_papers.append(paper)\n",
    "\n",
    "for paper in cleaned_papers:\n",
    "    print(paper)\n",
    "\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(cleaned_papers, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(f\"清理完成，共 {len(cleaned_papers)} 筆，已儲存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ce2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# 建立 DynamoDB 連線\n",
    "dynamodb = boto3.resource(\n",
    "    'dynamodb',\n",
    "    aws_access_key_id='',\n",
    "    aws_secret_access_key='',\n",
    "    region_name='ap-southeast-2'\n",
    ")\n",
    "\n",
    "# 指定 table 名稱\n",
    "table = dynamodb.Table('download_paper_entry_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf2a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已新增\n"
     ]
    }
   ],
   "source": [
    "# 新增一筆資料\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "entry_id = \"http://arxiv.org/abs/2510.11683v1\"\n",
    "item = {\n",
    "    \"category\": \"cs.LG\",\n",
    "    \"entry_id\": entry_id,\n",
    "    \"status\": \"uploaded\",  # \"failed\"\n",
    "    \"last_attempt\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"error_msg\": \"\" \n",
    "}\n",
    "\n",
    "try:\n",
    "    table.put_item(\n",
    "        Item=item,\n",
    "        ConditionExpression='attribute_not_exists(entry_id)'\n",
    "    )\n",
    "    print(\"已新增\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ConditionalCheckFailedException':\n",
    "        print(\"這篇 paper 已存在\")\n",
    "    else:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60adb45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已存在 {'category': 'cs.LG', 'last_attempt': '2025-10-16 09:34:28', 'error_msg': '', 'status': 'uploaded', 'entry_id': 'http://arxiv.org/abs/2510.11683v1'}\n"
     ]
    }
   ],
   "source": [
    "# key 查詢\n",
    "response = table.get_item(Key={\"category\": \"cs.LG\",'entry_id': entry_id})\n",
    "item = response.get('Item')\n",
    "\n",
    "if item:\n",
    "    print(\"已存在\", item)\n",
    "else:\n",
    "    print(\"不存在\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2961df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "刪除成功: {'ResponseMetadata': {'RequestId': '7TTA1C0VHABSMDGRPOCFBDJNCBVV4KQNSO5AEMVJF66Q9ASUAAJG', 'HTTPStatusCode': 200, 'HTTPHeaders': {'server': 'Server', 'date': 'Thu, 16 Oct 2025 09:36:05 GMT', 'content-type': 'application/x-amz-json-1.0', 'content-length': '2', 'connection': 'keep-alive', 'x-amzn-requestid': '7TTA1C0VHABSMDGRPOCFBDJNCBVV4KQNSO5AEMVJF66Q9ASUAAJG', 'x-amz-crc32': '2745614147'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# 刪除 key\n",
    "response = table.delete_item(\n",
    "    Key={\n",
    "        \"category\": \"cs.LG\",\n",
    "        \"entry_id\": entry_id\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"刪除成功:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "def create_s3_bucket_and_prefix(bucket_name: str, domain: str):\n",
    "    env_path = os.path.join(os.path.dirname(__file__), \"../.env\")\n",
    "    if not os.path.exists(env_path):\n",
    "        raise FileNotFoundError(f\".env not found at {env_path}\")\n",
    "    \n",
    "    load_dotenv(env_path)\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        region_name=os.getenv(\"AWS_REGION\"),\n",
    "        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    )\n",
    "\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "    prefix = f\"raw/domain={domain}/\"\n",
    "    s3.put_object(Bucket=bucket_name, Key=(prefix + \".keep\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_s3_bucket_and_prefix(\"my-test-bucket\", \"cs.LG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2ab9f8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hackmd-paper-bucket\n"
     ]
    }
   ],
   "source": [
    "# 查看你有哪個 Bucket\n",
    "\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "response = s3.list_buckets()\n",
    "for bucket in response[\"Buckets\"]:\n",
    "    print(bucket[\"Name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "407ee619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefixes:\n",
      "raw/domain=cs.LG/\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=\"raw/\",   # 只看 raw/ 底下\n",
    "    Delimiter=\"/\"\n",
    ")\n",
    "\n",
    "if \"CommonPrefixes\" in response:\n",
    "    print(\"Prefixes:\")\n",
    "    for prefix in response[\"CommonPrefixes\"]:\n",
    "        print(prefix[\"Prefix\"])\n",
    "else:\n",
    "    print(\"沒有找到任何 prefix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "68657f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已上傳 /home/hank/hackmd-data-pipeline/tests/arxiv_data/arxiv_batch_2.json 到 S3: raw/domain=cs.LG/arxiv_batch_2.json\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "prefix = \"raw/domain=cs.LG/\"\n",
    "local_file = \"/home/hank/hackmd-data-pipeline/tests/arxiv_data/arxiv_batch_2.json\"\n",
    "key = prefix + os.path.basename(local_file)\n",
    "\n",
    "with open(local_file, \"rb\") as f:\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=f,\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "print(f\"已上傳 {local_file} 到 S3: {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "302e5623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "檔案列表：\n",
      "raw/domain=cs.LG/arxiv_batch_2.json\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "prefix = \"raw/domain=cs.LG/\"\n",
    "\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=prefix,\n",
    "    Delimiter=\"/\" \n",
    ")\n",
    "\n",
    "if \"Contents\" in response:\n",
    "    print(\"檔案列表：\")\n",
    "    files = [obj[\"Key\"] for obj in response[\"Contents\"] if not obj[\"Key\"].endswith(\".keep\")]\n",
    "    for f in files:\n",
    "        print(f)\n",
    "else:\n",
    "    print(\"此 prefix 下沒有檔案\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackmd-data-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
