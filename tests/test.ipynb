{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c23e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取環境變數\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "sys.path.append(str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37300cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client(\n",
    "  page_size=1,\n",
    ")\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=\"\",\n",
    "    max_results=1,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    sort_order=arxiv.SortOrder.Descending,\n",
    "    id_list=[\"2301.12345v1\"]\n",
    ")\n",
    "\n",
    "result = next(client.results(search))\n",
    "for attr, value in vars(result).items():\n",
    "  print(attr,\":\",value)\n",
    "    \n",
    "# print(\"ID:\", result.entry_id)\n",
    "# print(\"標題:\", result.title)\n",
    "# print(\"摘要:\", result.summary)\n",
    "# print(\"作者:\", \", \".join(str(a) for a in result.authors))\n",
    "# print(\"主分類:\", result.primary_category)\n",
    "# print(\"其他分類:\", \", \".join(result.categories))\n",
    "# print(\"發表日期:\", result.published)\n",
    "# print(\"更新日期:\", result.updated)\n",
    "# print(\"期刊/會議資訊:\", result.journal_ref)\n",
    "# print(\"DOI:\", result.doi)\n",
    "# print(\"PDF_url:\", result.pdf_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a094a614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs.AI: Finished\n",
      "cs.LG: Finished\n",
      "cs.CV: Finished\n",
      "cs.CL: Finished\n",
      "cs.NE: Finished\n",
      "cs.RO: Finished\n",
      "cs.HC: Finished\n",
      "cs.SE: Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-17 02:46:56.520\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36msrc.core.pg_engine\u001b[0m:\u001b[36mclose_connect\u001b[0m:\u001b[36m133\u001b[0m - \u001b[31m\u001b[1m'NoneType' object has no attribute 'close'\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 189\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pg_batch)):\n\u001b[32m    188\u001b[39m     pg_batch[i] = (pg_batch[i][\u001b[32m0\u001b[39m], pg_batch[i][\u001b[32m1\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33muploaded\u001b[39m\u001b[33m\"\u001b[39m, pg_batch[i][\u001b[32m3\u001b[39m], pg_batch[i][\u001b[32m4\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[43mflush_pg_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m batch = []\n\u001b[32m    191\u001b[39m pg_batch = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mflush_pg_batch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[43mpg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert_mogrify\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpapers.downloaded_papers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpg_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    105\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to insert batch into Postgres: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/src/core/pg_engine.py:107\u001b[39m, in \u001b[36mPsqlEngine.insert_mogrify\u001b[39m\u001b[34m(self, table_name, values)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.conn:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mre_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cursor:\n\u001b[32m    109\u001b[39m         \u001b[38;5;28mself\u001b[39m.cursor = \u001b[38;5;28mself\u001b[39m.conn.cursor()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/src/core/pg_engine.py:55\u001b[39m, in \u001b[36mPsqlEngine.re_connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mre_connect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/src/core/pg_engine.py:44\u001b[39m, in \u001b[36mPsqlEngine.connect_db\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect_db\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28mself\u001b[39m.conn = \u001b[43mpsycopg2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdbname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdbname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mself\u001b[39m.conn.set_session(autocommit=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackmd-data-pipeline/.venv/lib/python3.11/site-packages/psycopg2/__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import time\n",
    "import time\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import gzip\n",
    "import io\n",
    "import boto3\n",
    "from datetime import datetime, timezone\n",
    "from src.core.db import get_pg\n",
    "from src.core.pg_engine import PsqlEngine\n",
    "\n",
    "pg = get_pg()\n",
    "load_dotenv(\"../.env\")\n",
    "logging.getLogger(\"arxiv\").setLevel(logging.WARNING)\n",
    "\n",
    "category_list = [\n",
    "    # Computer Science\n",
    "    \"cs.AI\", \"cs.LG\", \"cs.CV\", \"cs.CL\", \"cs.NE\", \"cs.RO\", \"cs.HC\", \"cs.SE\", \"cs.DS\",\n",
    "    \"cs.DB\", \"cs.SY\", \"cs.OS\", \"cs.PF\", \"cs.PL\", \"cs.MS\", \"cs.CC\", \"cs.CG\", \"cs.LO\",\n",
    "    \"cs.GT\", \"cs.MM\", \"cs.IT\",\n",
    "\n",
    "    # Mathematics\n",
    "    \"math.CO\", \"math.AG\", \"math.GT\", \"math.QA\", \"math.RA\", \"math.NT\", \"math.KT\", \"math.DG\",\n",
    "    \"math.DS\", \"math.FA\", \"math.AP\", \"math.SP\", \"math.ST\", \"math.PR\",\n",
    "\n",
    "    # Statistics\n",
    "    \"stat.ML\", \"stat.TH\", \"stat.CO\", \"stat.AP\", \"stat.OT\", \"stat.ME\",\n",
    "\n",
    "    # Physics\n",
    "    \"physics.optics\", \"physics.bio-ph\", \"physics.gen-ph\", \"physics.acc-ph\", \"physics.chem-ph\",\n",
    "    \"physics.class-ph\", \"physics.comp-ph\", \"physics.data-an\", \"physics.ed-ph\", \"physics.ins-det\",\n",
    "    \"physics.med-ph\", \"physics.plasm-ph\", \"physics.space-ph\",\n",
    "\n",
    "    # Quantitative Biology\n",
    "    \"q-bio.BM\", \"q-bio.CB\", \"q-bio.GN\", \"q-bio.MN\", \"q-bio.NC\", \"q-bio.PE\", \"q-bio.QM\", \"q-bio.SC\",\n",
    "\n",
    "    # Quantitative Finance\n",
    "    \"q-fin.CP\", \"q-fin.EC\", \"q-fin.GN\", \"q-fin.MF\", \"q-fin.PM\", \"q-fin.RM\", \"q-fin.ST\", \"q-fin.TR\",\n",
    "\n",
    "    # Electrical Engineering and Systems Science\n",
    "    \"eess.AS\", \"eess.IV\", \"eess.SP\", \"eess.SY\",\n",
    "\n",
    "    # Astrophysics\n",
    "    \"astro-ph.CO\", \"astro-ph.GA\", \"astro-ph.HE\", \"astro-ph.IM\", \"astro-ph.SR\",\n",
    "\n",
    "    # Condensed Matter Physics\n",
    "    \"cond-mat.mtrl-sci\", \"cond-mat.str-el\", \"cond-mat.supr-con\", \"cond-mat.quant-gas\",\n",
    "    \"cond-mat.dis-nn\", \"cond-mat.soft\", \"cond-mat.stat-mech\",\n",
    "\n",
    "    # High Energy Physics\n",
    "    \"hep-ex\", \"hep-lat\", \"hep-ph\", \"hep-th\"\n",
    "]\n",
    "\n",
    "MAX_RESULTS_GOAL = 1000\n",
    "BATCH_SIZE = 100\n",
    "S3_BUCKET = \"hackmd-paper-bucket\"\n",
    "MAX_ATTEMPTS = 3\n",
    "INITIAL_DELAY_SECONDS = 5\n",
    "\n",
    "client = arxiv.Client(\n",
    "    page_size=MAX_RESULTS_GOAL, \n",
    "    delay_seconds=3,\n",
    "    num_retries=3\n",
    ")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "def load_existing_ids():\n",
    "    stmt = f\"\"\"\n",
    "        SELECT entry_id\n",
    "        FROM papers.downloaded_papers\n",
    "    \"\"\"\n",
    "    rows = pg.execute_query(stmt)\n",
    "    return set(r[0] for r in rows)\n",
    "\n",
    "def add_to_pg_batch(category, entry_id, status, error_msg=\"\"):\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    pg_batch.append((category, entry_id, status, now_utc, error_msg))\n",
    "\n",
    "def paper_exists(pg: PsqlEngine, category: str, entry_id: str) -> bool:\n",
    "    stmt = f\"\"\"\n",
    "        SELECT 1\n",
    "        FROM papers.downloaded_papers\n",
    "        WHERE category = '{category}' AND entry_id = '{entry_id}'\n",
    "        LIMIT 1;\n",
    "    \"\"\"\n",
    "    result = pg.execute_query(stmt)\n",
    "    return bool(result)\n",
    "\n",
    "def flush_pg_batch():\n",
    "    global pg_batch\n",
    "    if not pg_batch:\n",
    "        return\n",
    "    try:\n",
    "        pg.insert_mogrify(\"papers.downloaded_papers\", pg_batch)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to insert batch into Postgres: {e}\")\n",
    "    finally:\n",
    "        pg_batch = []\n",
    "\n",
    "def upload_batch_to_s3(s3_prefix, batch_data, batch_num, category):\n",
    "    if not batch_data:\n",
    "        return\n",
    "    jsonl_content = \"\\n\".join([json.dumps(paper, ensure_ascii=False) for paper in batch_data])\n",
    "    buffer = io.BytesIO()\n",
    "    with gzip.GzipFile(fileobj=buffer, mode='wb') as f:\n",
    "        f.write(jsonl_content.encode('utf-8'))\n",
    "    gzip_bytes = buffer.getvalue()\n",
    "    \n",
    "    utc_now = datetime.now(timezone.utc)\n",
    "    today_str = utc_now.strftime(\"%Y-%m-%d\")\n",
    "    utc_timestamp = int(utc_now.timestamp())\n",
    "    s3_key = f\"{s3_prefix}{today_str}/{category.replace('.','_')}_batch_{batch_num}_{utc_timestamp}.jsonl.gz\"\n",
    "    \n",
    "    last_exception = None\n",
    "    for attempt in range(MAX_ATTEMPTS):\n",
    "        try:\n",
    "            s3.put_object(\n",
    "                Bucket=S3_BUCKET,\n",
    "                Key=s3_key,\n",
    "                Body=gzip_bytes,\n",
    "                ContentType='application/jsonl'\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_exception = e\n",
    "            if attempt < MAX_ATTEMPTS - 1:\n",
    "                time.sleep(INITIAL_DELAY_SECONDS * (2 ** attempt))\n",
    "            else:\n",
    "                raise last_exception\n",
    "\n",
    "existing_ids = load_existing_ids()\n",
    "category_stats = {}\n",
    "for category in category_list:\n",
    "    start_time = time.time()\n",
    "    s3_count = 0\n",
    "    pg_count = 0\n",
    "    try:\n",
    "        S3_PREFIX = f\"raw/{category.replace('.','_')}/\"\n",
    "        search = arxiv.Search(\n",
    "            query=f'cat:{category}',\n",
    "            max_results=MAX_RESULTS_GOAL,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "            sort_order=arxiv.SortOrder.Descending\n",
    "        )\n",
    "        batch = []\n",
    "        batch_ids = set()\n",
    "        pg_batch = []\n",
    "        batch_count = 0\n",
    "        total_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                results_generator = client.results(search, offset=total_count)\n",
    "                for paper_result in results_generator:\n",
    "                    entry_id = paper_result.entry_id\n",
    "                    if entry_id in existing_ids or entry_id in batch_ids:\n",
    "                        continue\n",
    "                    existing_ids.add(entry_id)\n",
    "                    batch_ids.add(entry_id)\n",
    "                    paper_data = {\n",
    "                        \"entry_id\": entry_id,\n",
    "                        \"title\": paper_result.title,\n",
    "                        \"authors\": [a.name for a in paper_result.authors],\n",
    "                        \"summary\": paper_result.summary,\n",
    "                        \"primary_category\": paper_result.primary_category,\n",
    "                        \"categories\": paper_result.categories,\n",
    "                        \"published\": paper_result.published.isoformat(),\n",
    "                        \"updated\": paper_result.updated.isoformat(),\n",
    "                        \"journal_ref\": paper_result.journal_ref,\n",
    "                        \"doi\": paper_result.doi\n",
    "                    }\n",
    "                    batch.append(paper_data)\n",
    "                    total_count += 1\n",
    "                    add_to_pg_batch(category, entry_id, \"pending\")\n",
    "                    pg_count += 1\n",
    "                    if len(batch) >= BATCH_SIZE:\n",
    "                        upload_batch_to_s3(S3_PREFIX, batch, batch_count, category)\n",
    "                        s3_count += len(batch)\n",
    "                        for i in range(len(pg_batch)):\n",
    "                            pg_batch[i] = (pg_batch[i][0], pg_batch[i][1], \"uploaded\", pg_batch[i][3], pg_batch[i][4])\n",
    "                        flush_pg_batch()\n",
    "                        batch = []\n",
    "                        pg_batch = []\n",
    "                        batch_count += 1\n",
    "                break\n",
    "            except arxiv.UnexpectedEmptyPageError:\n",
    "                total_count += 1\n",
    "                continue\n",
    "        if batch:\n",
    "            upload_batch_to_s3(S3_PREFIX, batch, batch_count, category)\n",
    "            s3_count += len(batch)\n",
    "            for i in range(len(pg_batch)):\n",
    "                pg_batch[i] = (pg_batch[i][0], pg_batch[i][1], \"uploaded\", pg_batch[i][3], pg_batch[i][4])\n",
    "            flush_pg_batch()\n",
    "        elapsed = time.time() - start_time\n",
    "        category_stats[category] = {\"time_sec\": elapsed, \"s3_count\": s3_count, \"pg_count\": pg_count}\n",
    "        print(f\"{category}: Finished\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during category {category}: {e}\")\n",
    "\n",
    "for cat, stats in category_stats.items():\n",
    "    print(f\"{cat} -> Time: {stats['time_sec']:.2f}s, S3: {stats['s3_count']}, PostgreSQL: {stats['pg_count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "input_file = Path(\"arxiv_data/arxiv_batch_1.json\")\n",
    "output_file = Path(\"arxiv_data/arxiv_batch_cleaned.json\")\n",
    "\n",
    "def transform_datetime2date(dt_str):\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(dt_str.replace(\"Z\", \"+00:00\"))\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "# 去重\n",
    "unique_papers = {paper[\"entry_id\"]: paper for paper in papers}\n",
    "\n",
    "\n",
    "required_fields = [\n",
    "    \"entry_id\", \"title\", \"summary\", \"authors\", \n",
    "    \"primary_category\", \"published\", \"updated\"\n",
    "]\n",
    "\n",
    "cleaned_papers = []\n",
    "for paper in unique_papers.values():\n",
    "    # 刪除缺值資料\n",
    "    if all(paper.get(field) for field in required_fields) and all(a.strip() for a in paper[\"authors\"]):\n",
    "        paper[\"published_date\"] = transform_datetime2date(paper[\"published\"])\n",
    "        paper[\"updated_date\"] = transform_datetime2date(paper[\"updated\"])\n",
    "        paper[\"etl_datetime\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\") # use UTC timezone\n",
    "        cleaned_papers.append(paper)\n",
    "\n",
    "for paper in cleaned_papers:\n",
    "    print(paper)\n",
    "\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(cleaned_papers, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(f\"清理完成，共 {len(cleaned_papers)} 筆，已儲存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ce2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# 建立 DynamoDB 連線\n",
    "dynamodb = boto3.resource(\n",
    "    \"dynamodb\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "table = dynamodb.Table('download_paper_entry_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新增一筆資料\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "entry_id = \"http://arxiv.org/abs/2510.11683v1\"\n",
    "item = {\n",
    "    \"category\": \"cs.LG\",\n",
    "    \"entry_id\": entry_id,\n",
    "    \"status\": \"uploaded\",  # \"failed\"\n",
    "    \"last_attempt\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"error_msg\": \"\" \n",
    "}\n",
    "\n",
    "try:\n",
    "    table.put_item(\n",
    "        Item=item,\n",
    "        ConditionExpression='attribute_not_exists(entry_id)'\n",
    "    )\n",
    "    print(\"已新增\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ConditionalCheckFailedException':\n",
    "        print(\"這篇 paper 已存在\")\n",
    "    else:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60adb45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key 查詢\n",
    "response = table.get_item(Key={\"category\": \"cs.LG\",'entry_id': entry_id})\n",
    "item = response.get('Item')\n",
    "\n",
    "if item:\n",
    "    print(\"已存在\", item)\n",
    "else:\n",
    "    print(\"不存在\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2961df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除 key\n",
    "response = table.delete_item(\n",
    "    Key={\n",
    "        \"category\": \"cs.LG\",\n",
    "        \"entry_id\": entry_id\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"刪除成功:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "def create_s3_bucket_and_prefix(bucket_name: str, domain: str):\n",
    "    env_path = os.path.join(os.path.dirname(__file__), \"../.env\")\n",
    "    if not os.path.exists(env_path):\n",
    "        raise FileNotFoundError(f\".env not found at {env_path}\")\n",
    "    \n",
    "    load_dotenv(env_path)\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        region_name=os.getenv(\"AWS_REGION\"),\n",
    "        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    )\n",
    "\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "    prefix = f\"raw/domain={domain}/\"\n",
    "    s3.put_object(Bucket=bucket_name, Key=(prefix + \".keep\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_s3_bucket_and_prefix(\"my-test-bucket\", \"cs.LG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看你有哪個 Bucket\n",
    "\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "response = s3.list_buckets()\n",
    "for bucket in response[\"Buckets\"]:\n",
    "    print(bucket[\"Name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ee619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=\"raw/\",   # 只看 raw/ 底下\n",
    "    Delimiter=\"/\"\n",
    ")\n",
    "\n",
    "if \"CommonPrefixes\" in response:\n",
    "    print(\"Prefixes:\")\n",
    "    for prefix in response[\"CommonPrefixes\"]:\n",
    "        print(prefix[\"Prefix\"])\n",
    "else:\n",
    "    print(\"沒有找到任何 prefix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68657f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "prefix = \"raw/domain=cs.LG/\"\n",
    "local_file = \"/home/hank/hackmd-data-pipeline/tests/arxiv_data/arxiv_batch_2.json\"\n",
    "key = prefix + os.path.basename(local_file)\n",
    "\n",
    "with open(local_file, \"rb\") as f:\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=f,\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "print(f\"已上傳 {local_file} 到 S3: {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=os.getenv(\"AWS_REGION\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket_name = \"hackmd-paper-bucket\"\n",
    "prefix = \"raw/domain=cs.LG/\"\n",
    "\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=prefix,\n",
    "    Delimiter=\"/\" \n",
    ")\n",
    "\n",
    "if \"Contents\" in response:\n",
    "    print(\"檔案列表：\")\n",
    "    files = [obj[\"Key\"] for obj in response[\"Contents\"] if not obj[\"Key\"].endswith(\".keep\")]\n",
    "    for f in files:\n",
    "        print(f)\n",
    "else:\n",
    "    print(\"此 prefix 下沒有檔案\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.db import get_pg\n",
    "\n",
    "pg = get_pg()\n",
    "\n",
    "def paper_exists(pg: PsqlEngine, category: str, entry_id: str) -> bool:\n",
    "    stmt = f\"\"\"\n",
    "        SELECT 1\n",
    "        FROM papers.downloaded_papers\n",
    "        WHERE category = '{category}' AND entry_id = '{entry_id}'\n",
    "        LIMIT 1;\n",
    "    \"\"\"\n",
    "    result = pg.execute_query(stmt)\n",
    "    return bool(result)\n",
    "\n",
    "\n",
    "paper_exists(pg, \"cs_LG\",'dsfd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
