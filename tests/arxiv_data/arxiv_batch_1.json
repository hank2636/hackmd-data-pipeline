[
  {
    "id": "http://arxiv.org/abs/2510.09599v1",
    "title": "Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation",
    "authors": [
      "Sondos Mahmoud Bsharat",
      "Zhiqiang Shen"
    ],
    "summary": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities when provided with chain-of-thought exemplars, but curating large\nreasoning datasets remains laborious and resource-intensive. In this work, we\nintroduce Prompting Test-Time Scaling (P-TTS), a simple yet effective\ninference-time data augmentation strategy for enhancing LLM reasoning through\nfinetuning. Rather than collecting thousands or even millions of examples,\nP-TTS leverages a small pool of only 90 manually selected reasoning instances\nand systematically varies exemplar augmentation through principled instruction\nprompting intensities at test time to synthesize diverse reasoning trajectory\ncontexts. Then we finetune the various sizes of Qwen-2.5 models on P-TTS data.\nAcross a suite of mathematical reasoning AIME2024 & 25, MATH500, and\nGPQA-Diamond, our P-TTS-7B and 32B models outperform the prior competitive\nbaselines like S1 and S1.1 (1K-shot), achieving absolute accuracy gains of\n+26.66% and +30.00% on AIME'24 (7B), and +13.34% and +6.67% on AIME'25 (7B);\nP-TTS-32B yields gains of +23.33% and +16.63% on AIME'24, and +26.63% and\n+3.33% on AIME'25 (vs. S1 and S1.1, respectively), with comparable or better\nperformance on MATH500 and GPQA-Diamond. We further show that P-TTS enhances\nzero-shot generalization accuracy on out-of-domain reasoning benchmarks of\nGaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Our\nanalysis suggests that test-time scaling effectively explores the latent space\nof reasoning patterns, amplifying LLM problem-solving with minimal annotation\noverhead, and further unlocking the reasoning potential and capabilities of\nLLMs. Prompting Test-Time Scaling offers a practical, low-cost way to elicit\nLLM reasoning in resource-constrained or rapidly evolving domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-10T17:57:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09596v1",
    "title": "BaNEL: Exploration Posteriors for Generative Modeling Using Only Negative Rewards",
    "authors": [
      "Sangyun Lee",
      "Brandon Amos",
      "Giulia Fanti"
    ],
    "summary": "Today's generative models thrive with large amounts of supervised data and\ninformative reward functions characterizing the quality of the generation. They\nwork under the assumptions that the supervised data provides knowledge to\npre-train the model, and the reward function provides dense information about\nhow to further improve the generation quality and correctness. However, in the\nhardest instances of important problems, two problems arise: (1) the base\ngenerative model attains a near-zero reward signal, and (2) calls to the reward\noracle are expensive. This setting poses a fundamentally different learning\nchallenge than standard reward-based post-training. To address this, we propose\nBaNEL (Bayesian Negative Evidence Learning), an algorithm that post-trains the\nmodel using failed attempts only, while minimizing the number of reward\nevaluations (NREs). Our method is based on the idea that the problem of\nlearning regularities underlying failures can be cast as another, in-loop\ngenerative modeling problem. We then leverage this model to assess whether new\ndata resembles previously seen failures and steer the generation away from\nthem. We show that BaNEL can improve model performance without observing a\nsingle successful sample on several sparse-reward tasks, outperforming existing\nnovelty-bonus approaches by up to several orders of magnitude in success rate,\nwhile using fewer reward evaluations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-10-10T17:55:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09595v1",
    "title": "LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?",
    "authors": [
      "Kaijian Zou",
      "Aaron Xiong",
      "Yunxiang Zhang",
      "Frederick Zhang",
      "Yueqi Ren",
      "Jirong Yang",
      "Ayoung Lee",
      "Shitanshu Bhushan",
      "Lu Wang"
    ],
    "summary": "Competitive programming problems increasingly serve as valuable benchmarks to\nevaluate the coding capabilities of large language models (LLMs) due to their\ncomplexity and ease of verification. Yet, current coding benchmarks face\nlimitations such as lack of exceptionally challenging problems, insufficient\ntest case coverage, reliance on online platform APIs that limit accessibility.\nTo address these issues, we introduce LiveOIBench, a comprehensive benchmark\nfeaturing 403 expert-curated Olympiad-level competitive programming problems,\neach with an average of 60 expert-designed test cases. The problems are sourced\ndirectly from 72 official Informatics Olympiads in different regions conducted\nbetween 2023 and 2025. LiveOIBench distinguishes itself through four key\nfeatures: (1) meticulously curated high-quality tasks with detailed subtask\nrubrics and extensive private test cases; (2) direct integration of elite\ncontestant performance data to enable informative comparison against\ntop-performing humans; (3) planned continuous, contamination-free updates from\nnewly released Olympiad problems; and (4) a self-contained evaluation system\nfacilitating offline and easy-to-reproduce assessments. Benchmarking 32 popular\ngeneral-purpose and reasoning LLMs, we find that GPT-5 achieves a notable\n81.76th percentile, a strong result that nonetheless falls short of top human\ncontestant performance, who usually place above 90th. In contrast, among\nopen-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile,\nunderscoring significant capability disparities from frontier closed models.\nDetailed analyses indicate that robust reasoning models prioritize precise\nproblem analysis over excessive exploration, suggesting future models should\nemphasize structured analysis and minimize unnecessary exploration. All data,\ncode, and leaderboard results will be made publicly available on our website.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-10-10T17:54:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09594v1",
    "title": "MODE: Learning compositional representations of complex systems with Mixtures Of Dynamical Experts",
    "authors": [
      "Nathan Quiblier",
      "Roy Friedman",
      "Matthew Ricci"
    ],
    "summary": "Dynamical systems in the life sciences are often composed of complex mixtures\nof overlapping behavioral regimes. Cellular subpopulations may shift from\ncycling to equilibrium dynamics or branch towards different developmental\nfates. The transitions between these regimes can appear noisy and irregular,\nposing a serious challenge to traditional, flow-based modeling techniques which\nassume locally smooth dynamics. To address this challenge, we propose MODE\n(Mixture Of Dynamical Experts), a graphical modeling framework whose neural\ngating mechanism decomposes complex dynamics into sparse, interpretable\ncomponents, enabling both the unsupervised discovery of behavioral regimes and\naccurate long-term forecasting across regime transitions. Crucially, because\nagents in our framework can jump to different governing laws, MODE is\nespecially tailored to the aforementioned noisy transitions. We evaluate our\nmethod on a battery of synthetic and real datasets from computational biology.\nFirst, we systematically benchmark MODE on an unsupervised classification task\nusing synthetic dynamical snapshot data, including in noisy, few-sample\nsettings. Next, we show how MODE succeeds on challenging forecasting tasks\nwhich simulate key cycling and branching processes in cell biology. Finally, we\ndeploy our method on human, single-cell RNA sequencing data and show that it\ncan not only distinguish proliferation from differentiation dynamics but also\npredict when cells will commit to their ultimate fate, a key outstanding\nchallenge in computational biology.",
    "categories": [
      "cs.LG",
      "q-bio.MN",
      "37, 60",
      "I.6.0; J.3"
    ],
    "published": "2025-10-10T17:52:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09593v1",
    "title": "STaTS: Structure-Aware Temporal Sequence Summarization via Statistical Window Merging",
    "authors": [
      "Disharee Bhowmick",
      "Ranjith Ramanathan",
      "Sathyanarayanan N. Aakur"
    ],
    "summary": "Time series data often contain latent temporal structure, transitions between\nlocally stationary regimes, repeated motifs, and bursts of variability, that\nare rarely leveraged in standard representation learning pipelines. Existing\nmodels typically operate on raw or fixed-window sequences, treating all time\nsteps as equally informative, which leads to inefficiencies, poor robustness,\nand limited scalability in long or noisy sequences. We propose STaTS, a\nlightweight, unsupervised framework for Structure-Aware Temporal Summarization\nthat adaptively compresses both univariate and multivariate time series into\ncompact, information-preserving token sequences. STaTS detects change points\nacross multiple temporal resolutions using a BIC-based statistical divergence\ncriterion, then summarizes each segment using simple functions like the mean or\ngenerative models such as GMMs. This process achieves up to 30x sequence\ncompression while retaining core temporal dynamics. STaTS operates as a\nmodel-agnostic preprocessor and can be integrated with existing unsupervised\ntime series encoders without retraining. Extensive experiments on 150+\ndatasets, including classification tasks on the UCR-85, UCR-128, and UEA-30\narchives, and forecasting on ETTh1 and ETTh2, ETTm1, and Electricity,\ndemonstrate that STaTS enables 85-90\\% of the full-model performance while\noffering dramatic reductions in computational cost. Moreover, STaTS improves\nrobustness under noise and preserves discriminative structure, outperforming\nuniform and clustering-based compression baselines. These results position\nSTaTS as a principled, general-purpose solution for efficient, structure-aware\ntime series modeling.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2025-10-10T17:51:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09591v1",
    "title": "A Multilingual Python Programming Language",
    "authors": [
      "Saad Ahmed Bazaz",
      "Mirza Omer Beg"
    ],
    "summary": "All widely used and useful programming languages have a common problem. They\nrestrict entry on the basis of knowledge of the English language. The lack of\nknowledge of English poses a major hurdle to many newcomers who do not have the\nresources, in terms of time and money, to learn the English language. Studies\nshow that people learn better in their own language. Therefore, we propose a\nlanguage transpiler built on top of the Python programming language, called\nUniversalPython, which allows one to write Python in their own human language.\nWe demonstrate the ability to create an \"Urdu Python\" with this transpiler. In\nthe future, we aim to scale the language to encapsulate more human languages to\nincrease the availability of programming. The source code for this transpiler\nis open-source, and available at\nhttps://github.com/universalpython/universalpython",
    "categories": [
      "cs.PL"
    ],
    "published": "2025-10-10T17:49:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09589v1",
    "title": "Minimizing the Weighted Makespan with Restarts on a Single Machine",
    "authors": [
      "Aflatoun Amouzandeh",
      "Klaus Jansen",
      "Lis Pirotton",
      "Rob van Stee",
      "Corinna Wambsganz"
    ],
    "summary": "We consider the problem of minimizing the weighted makespan on a single\nmachine with restarts. Restarts are similar to preemptions but weaker: a job\ncan be interrupted, but then it has to be run again from the start instead of\nresuming at the point of interruption later. The objective is to minimize the\nweighted makespan, defined as the maximum weighted completion time of jobs.\n  We establish a lower bound of 1.4656 on the competitive ratio achievable by\ndeterministic online algorithms. For the case where all jobs have identical\nprocessing times, we design and analyze a deterministic online algorithm that\nimproves the competitive ratio to better than 1.3098. Finally, we prove a lower\nbound of 1.2344 for this case.",
    "categories": [
      "cs.DS",
      "F.2.2"
    ],
    "published": "2025-10-10T17:45:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09586v1",
    "title": "Vision Language Models: A Survey of 26K Papers",
    "authors": [
      "Fengming Lin"
    ],
    "summary": "We present a transparent, reproducible measurement of research trends across\n26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles\nand abstracts are normalized, phrase-protected, and matched against a\nhand-crafted lexicon to assign up to 35 topical labels and mine fine-grained\ncues about tasks, architectures, training regimes, objectives, datasets, and\nco-mentioned modalities. The analysis quantifies three macro shifts: (1) a\nsharp rise of multimodal vision-language-LLM work, which increasingly reframes\nclassic perception as instruction following and multi-step reasoning; (2)\nsteady expansion of generative methods, with diffusion research consolidating\naround controllability, distillation, and speed; and (3) resilient 3D and video\nactivity, with composition moving from NeRFs to Gaussian splatting and a\ngrowing emphasis on human- and agent-centric understanding. Within VLMs,\nparameter-efficient adaptation like prompting/adapters/LoRA and lightweight\nvision-language bridges dominate; training practice shifts from building\nencoders from scratch to instruction tuning and finetuning strong backbones;\ncontrastive objectives recede relative to cross-entropy/ranking and\ndistillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and\nICLR the highest VLM share, while reliability themes such as efficiency or\nrobustness diffuse across areas. We release the lexicon and methodology to\nenable auditing and extension. Limitations include lexicon recall and\nabstract-only scope, but the longitudinal signals are consistent across venues\nand years.",
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-10T17:43:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09583v1",
    "title": "FSP-DETR: Few-Shot Prototypical Parasitic Ova Detection",
    "authors": [
      "Shubham Trehan",
      "Udhav Ramachandran",
      "Akash Rao",
      "Ruth Scimeca",
      "Sathyanarayanan N. Aakur"
    ],
    "summary": "Object detection in biomedical settings is fundamentally constrained by the\nscarcity of labeled data and the frequent emergence of novel or rare\ncategories. We present FSP-DETR, a unified detection framework that enables\nrobust few-shot detection, open-set recognition, and generalization to unseen\nbiomedical tasks within a single model. Built upon a class-agnostic DETR\nbackbone, our approach constructs class prototypes from original support images\nand learns an embedding space using augmented views and a lightweight\ntransformer decoder. Training jointly optimizes a prototype matching loss, an\nalignment-based separation loss, and a KL divergence regularization to improve\ndiscriminative feature learning and calibration under scarce supervision.\nUnlike prior work that tackles these tasks in isolation, FSP-DETR enables\ninference-time flexibility to support unseen class recognition, background\nrejection, and cross-task adaptation without retraining. We also introduce a\nnew ova species detection benchmark with 20 parasite classes and establish\nstandardized evaluation protocols. Extensive experiments across ova, blood\ncell, and malaria detection tasks demonstrate that FSP-DETR significantly\noutperforms prior few-shot and prototype-based detectors, especially in\nlow-shot and open-set scenarios.",
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-10T17:38:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09580v1",
    "title": "GraphMERT: Efficient and Scalable Distillation of Reliable Knowledge Graphs from Unstructured Data",
    "authors": [
      "Margarita Belova",
      "Jiaxin Xiao",
      "Shikhar Tuli",
      "Niraj K. Jha"
    ],
    "summary": "Researchers have pursued neurosymbolic artificial intelligence (AI)\napplications for nearly three decades because symbolic components provide\nabstraction while neural components provide generalization. Thus, a marriage of\nthe two components can lead to rapid advancements in AI. Yet, the field has not\nrealized this promise since most neurosymbolic AI frameworks fail to scale. In\naddition, the implicit representations and approximate reasoning of neural\napproaches limit interpretability and trust. Knowledge graphs (KGs), a\ngold-standard representation of explicit semantic knowledge, can address the\nsymbolic side. However, automatically deriving reliable KGs from text corpora\nhas remained an open problem. We address these challenges by introducing\nGraphMERT, a tiny graphical encoder-only model that distills high-quality KGs\nfrom unstructured text corpora and its own internal representations. GraphMERT\nand its equivalent KG form a modular neurosymbolic stack: neural learning of\nabstractions; symbolic KGs for verifiable reasoning. GraphMERT + KG is the\nfirst efficient and scalable neurosymbolic model to achieve state-of-the-art\nbenchmark accuracy along with superior symbolic representations relative to\nbaselines.\n  Concretely, we target reliable domain-specific KGs that are both (1) factual\n(with provenance) and (2) valid (ontology-consistent relations with\ndomain-appropriate semantics). When a large language model (LLM), e.g.,\nQwen3-32B, generates domain-specific KGs, it falls short on reliability due to\nprompt sensitivity, shallow domain expertise, and hallucinated relations. On\ntext obtained from PubMed papers on diabetes, our 80M-parameter GraphMERT\nyields a KG with a 69.8% FActScore; a 32B-parameter baseline LLM yields a KG\nthat achieves only 40.2% FActScore. The GraphMERT KG also attains a higher\nValidityScore of 68.8%, versus 43.0% for the LLM baseline.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-10T17:36:14+00:00"
  }
]